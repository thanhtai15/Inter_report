[{"uri":"https://thanhtai15.github.io/Inter_report/5-workshop/5.4-databaseandstorage/5.4.1-create-rds/","title":"Create RDS","tags":[],"description":"","content":" Open the Amazon Aurora and RDS\nIn left navbar, choose Databases, then click Create database In create console, choose Full configuration\nThen choose database type is MySQL\nChoose Templates is Production Select Availability and durability is Multi-AZ DB cluster deployment (3 instances)\nFill DB instance identifier\nChoose Self managed\nSpacific Master password In Instance configuration, choose db.m5d.large\nIn Storage, Allocated storage fill in 100 and Provisioned IOPS is 1000\nIn Connectivity, choose Don\u0026rsquo;t connect to an EC2, then select VPC Choose DB subnet group and then in VPC security group (firewall) select Choose existing\nSelect rds-sg\nThen click Create database "},{"uri":"https://thanhtai15.github.io/Inter_report/5-workshop/5.3-vpc/5.3.1-create-route-table/","title":"Create Route table and Internet Gateway","tags":[],"description":"","content":" A Route Table is a set of rules (routes) used by a router to determine the best path for data packets to reach their destination.\nOpen the Amazon VPC console Choose Route tables, then click Create route table In the Create route table console: Specify name of Route table Choose VPC created Then click Create route table In Route table console, click Route table created Choose Route in navbar -\u0026gt; click Edit routes Edit routes console -\u0026gt; choose Target local -\u0026gt; Save changes Choose Internet Gateway in left navbar -\u0026gt; click Create internet gateway In Create Internet Gateway Specify name of Internet Gateway Click Create internet gateway Back to Route table -\u0026gt; Create route table like step 3 Click this Route table -\u0026gt; Edit routes In Target choose Internet Gateway created Then click **Save changes "},{"uri":"https://thanhtai15.github.io/Inter_report/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"ApexEV — Workshop Introduction (Short) ApexEV is an enterprise-grade EV garage management platform that digitizes workshop operations, improves customer experience, and helps technicians work faster and safer.\nWhy this workshop:\nSolve common garage problems: manual processes, poor transparency, weak customer care, and data risk. Build a secure, scalable and cost-efficient cloud backend from day one using AWS best practices. Core architecture (summary):\nFrontend: React app hosted on AWS Amplify (CI/CD + CloudFront). Backend: Spring Boot services in ECS (Fargate) — serverless containers. Database: Amazon RDS (private subnets, automated backups, KMS encryption). Storage: Amazon S3 for media (use presigned URLs for direct uploads). API + Async: API Gateway as HTTPS ingress; SNS → Lambda → SES for email; Lambda → Bedrock for AI/chat. Network \u0026amp; Security: VPC (public/private), Security Groups, VPC Endpoints, WAF and least-privilege IAM. Key benefits:\nSecurity-first: backend and DB remain private; edge services terminate TLS and enforce WAF/rate limits. Cost-aware: Fargate + Lambda (pay-per-use), lifecycle rules and autoscaling reduce costs. Modern \u0026amp; modular: frontend/backend separation, event-driven async flows for resilience and scale. Workshop goals:\nProvision network and secure services, deploy frontend + backend, connect RDS and S3, and integrate email and AI pipelines. Each module includes steps, recommended settings and cleanup instructions. "},{"uri":"https://thanhtai15.github.io/Inter_report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Integrate into the AWS work culture and get to know new colleagues during the OJT process at First Cloud Journey (FCJ). Understand foundational knowledge of AWS Core Services. Practice operations on the AWS Management Console. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon - Preliminary research on AWS - Learn about the rules and regulations at the internship unit 09/08/2025 09/08/2025 Tue - Learn AWS through Module 1 + AWS Infrastructure + AWS Management Tools + Cost Optimization Practice: Create an AWS account 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/watch?v=HxYZAK1coOI https://www.youtube.com/watch?v=IK59Zdd1poE https://www.youtube.com/watch?v=HSzrWGqo3ME https://www.youtube.com/watch?v=pjr5a-HYAjI https://www.youtube.com/watch?v=2PQYqH_HkXw https://www.youtube.com/watch?v=IY61YlmXQe8 https://www.youtube.com/watch?v=Hku7exDBURo Wed - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Interact with AWS Console + Set up MFA and IAM + Install \u0026amp; configure AWS CLI 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ Thu - Learn about EC2: + Instance Types + Storage: EBS (Elastic Block Store) and Instance Store (Ephemeral Storage) + Network \u0026amp; Security: Security Groups, Key Pairs, and Public IP vs Private IP vs Elastic IP + IAM Role + Learn about SSH 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ Fri - Practice: + Launch an EC2 instance + Connect to that EC2 instance via SSH 09/12/2025 09/12/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Mastered the rules and working regulations at the FCJ internship unit.\nUnderstood the overview of AWS global infrastructure architecture, management tools, and cost optimization principles (Module 1).\nSuccessfully created and activated an AWS Free Tier account.\nFamiliarized with the AWS Management Console, performed account security setup:\nConfigured MFA (Multi-Factor Authentication). Created and managed basic IAM Users/Groups. Successfully installed and configured AWS CLI on a personal computer.\nMastered foundational knowledge of the Amazon EC2 service, including:\nInstance Types classification. Storage options: Distinguish between EBS (Elastic Block Store) and Instance Store. Network \u0026amp; Security mechanisms: Security Groups, Key Pairs, distinguishing Public/Private/Elastic IP. The role of IAM Role in access delegation. Successfully practiced with EC2 service:\nLaunched a complete EC2 Instance. Used SSH protocol to connect and interact with the Instance from the workstation. "},{"uri":"https://thanhtai15.github.io/Inter_report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"The internship roadmap was divided into three main phases:\nFoundational Knowledge: Mastering AWS core services (Networking, Compute, Storage, Security) and Math for ML. Advanced Research: Deep diving into NLP, Transformers, and Generative AI (AWS Bedrock). Project Implementation: System architecture design, Full-stack development, and Serverless deployment. Below is the weekly breakdown of tasks and achievements:\nWeek 1: Introduction to Cloud Computing \u0026amp; AWS Account Setup\nWeek 2: Deep Dive into AWS Networking: VPC \u0026amp; Security\nWeek 3: Compute, Connectivity (Transit Gateway) \u0026amp; Intro to NLP\nWeek 4: Hybrid Cloud Solutions, Storage Gateway \u0026amp; Math for NLP\nWeek 5: Advanced Storage, Security Automation \u0026amp; NLP Algorithms\nWeek 6: Cloud Architecture Design, Identity Security \u0026amp; Attention Models\nWeek 7: Databases, AI Chatbot Prototyping \u0026amp; Project Kick-off\nWeek 8: System Integration, Midterm Exam \u0026amp; Project Proposal\nWeek 9: Transformers, LLMs \u0026amp; Generative AI with AWS Bedrock\nWeek 10: BERT Fine-tuning, Serverless Optimization \u0026amp; UI Completion\nWeek 11: Documentation, Final Deployment \u0026amp; Architectural Synchronization\nWeek 12: Final Review, Bug Fixing \u0026amp; Product Demo\n"},{"uri":"https://thanhtai15.github.io/Inter_report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"European Region Announcement for Amazon Q Developer By Brian Beach | April 14, 2025 | in Amazon Q Developer, Announcements, Regions\nAs I sit down to write this post, my daughter just called from the top of the Eiffel Tower during her senior class trip. While she excitedly points her camera toward the Paris skyline, I suddenly realize how technology has changed our concept of distance. Her world, at eighteen, is infinitely more connected than the world I knew at her age. I can\u0026rsquo;t help but smile at the timing of this call, because today Amazon Q Developer is expanding to Europe.\nThe launch of Amazon Q Developer Pro Tier in the Frankfurt region (eu-central-1) marks an important milestone for our European customers, addressing two critical needs: data residency and performance optimization. For organizations that need to meet EU data residency requirements, the ability to store customer content within EU boundaries can help provide the assurances they require. Beyond compliance, the presence in this region also brings performance benefits. European customers will experience reduced latency in their interactions with Amazon Q Developer, as requests are processed closer to home. This proximity not only improves response times but also enhances the overall development experience, making real-time interactions with Amazon Q Developer more fluid and natural.\nAmazon Q Developer Pro tier users now have the choice to create profiles in N. Virginia (us-east-1) or Frankfurt (eu-central-1). Your content, including customizations, is stored in this region. Although your content is stored in Frankfurt, Amazon Q still uses multi-region inference to optimize request processing. At launch, this includes Frankfurt, Ireland, Paris, and Stockholm, as shown in the image below.\nMulti-Region Request Processing Finally, it\u0026rsquo;s important to note that certain operations, such as querying AWS resources in other regions (e.g., \u0026ldquo;List my S3 buckets in Tokyo\u0026rdquo;), will naturally involve multi-region calls regardless of where your Q Developer profile is located.\nThe Frankfurt region includes all GA features except command line and the ability to chat with Support. You can read more in the Amazon Q Developer User Guide.\nConclusion We invite you to experience these new capabilities by upgrading to the Pro tier and selecting Frankfurt as your region during profile creation. Get started with Amazon Q Developer, and share your feedback with us as we continue to expand our global presence.\n"},{"uri":"https://thanhtai15.github.io/Inter_report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Combining Snyk Insights with Amazon Q Developer Support to Streamline Secure Development By Omar Faruk and David Schott | April 28, 2025 | in Amazon Q Developer, Developer Tools, Partner solutions, Security\nToday, developers face a constant balancing act building new features and functionality while ensuring the security and reliability of their codebase. Two powerful tools, Snyk and Amazon Q Developer, can work in tandem to help developers navigate this challenge with greater efficiency and productivity.\nSnyk is a leading developer security platform that empowers developers to seamlessly secure their code, open-source dependencies, container images, and cloud infrastructure, all from a single unified platform. Amazon Q Developer is a generative AI-powered assistant designed to accelerate various tasks throughout the software development lifecycle. By combining security insights from Snyk with Amazon Q Developer\u0026rsquo;\u0026rsquo;s assistance capabilities, developers can streamline their workflows and focus on shipping products.\nGetting Started with Amazon Q Developer and Snyk IDE Plugins To get started with Amazon Q Developer, you need to have an AWS Builder ID or be part of an organization with an AWS IAM Identity Center instance that enables you to use Amazon Q. To use Amazon Q Developer agents for software development in Visual Studio Code, start by installing the Amazon Q extension. Find the latest version of the extension on the Amazon Q Developer page. This extension is also available for JetBrains IDEs, Eclipse (Preview), and Visual Studio. For a detailed list of supported IDEs and features available in each IDE, refer to the Amazon Q Developer documentation.\nTo get started with Snyk, sign up for a free Snyk account or log in with your existing account. To use Snyk in your IDE to automatically find security issues, review the IDE documentation and install Snyk using your IDE\u0026rsquo;\u0026rsquo;s extension marketplace. After Snyk is installed, navigate to the Snyk panel in your IDE and follow the on-screen instructions to authenticate with your Snyk account.\nAfter authentication, Snyk will automatically scan your entire codebase for security issues. Snyk will continue to scan periodically as you write code or generate code using Amazon Q Developer.\nWalkthrough Let\u0026rsquo;\u0026rsquo;s explore how Snyk and Amazon Q Developer can be used together through a few examples. Imagine you\u0026rsquo;\u0026rsquo;re maintaining an open-source project. As a new Snyk user, you want to find and fix security issues in the project. In this first, simple scenario, Snyk has identified multiple instances of security vulnerabilities in specific lines of code. Among the vulnerabilities, we\u0026rsquo;\u0026rsquo;ll focus on the Information Exposure vulnerability.\nFigure 1 Snyk IDE Plugin displaying vulnerability analysis of the Information Exposure issue, showing severity, affected code, and mitigation tips.\nInstead of researching and implementing the fix yourself, you simply highlight the flagged line of code, invoke Amazon Q Developer\u0026rsquo;\u0026rsquo;s inline chat by pressing +I (Mac) or Ctrl+I (Windows), and ask for assistance. Amazon Q Developer will analyze the issue, suggest necessary code changes, and provide you with an inline diff to review and accept. This enables quick remediation of security bugs, saving time while improving the code.\nFigure 2 Triggering Amazon Q Developer\u0026rsquo;\u0026rsquo;s inline code generation feature to fix the detected information exposure vulnerability.\nWe\u0026rsquo;\u0026rsquo;re happy with the change Amazon Q Developer has suggested, so we simply press enter to accept the suggestions. Of course, we can always press escape to reject the suggestion if needed.\nFigure 3 Amazon Q Developer displaying the inline code generation process to fix the detected information exposure vulnerability.\nUsing the /dev Agent Beyond inline chat, you can pass vulnerability details directly from the Snyk plugin\u0026rsquo;\u0026rsquo;s Problems view to Amazon Q Developer\u0026rsquo;\u0026rsquo;s /dev agent capability.\nIn Q Developer\u0026rsquo;\u0026rsquo;s chat interface, the /dev agent capability allows for longer conversations, broader workspace context, and handling changes across multiple files and topics. When this workflow is invoked, the Amazon Q Developer Agent generates code based on the description and existing code in the workspace, provides a list of suggestions to review and add to the workspace, and if needed, iterates on code generation based on feedback.\nFigure 4 Using Amazon Q\u0026rsquo;\u0026rsquo;s /dev agent to implement project-wide fixes for vulnerabilities detected by Snyk across multiple files.\nHandling Complex Issues Not all issues are as straightforward as the previous example. In a more complex case, Snyk may expose a vulnerability that requires deeper understanding of the code and potential risks. Let\u0026rsquo;\u0026rsquo;s consider another issue that Snyk has identified in the project we\u0026rsquo;\u0026rsquo;re discussing.\nFigure 5 Snyk Plugin highlighting a cross-site scripting (XSS) vulnerability, showing affected code lines and mitigation recommendations.\nHere, you can turn to Amazon Q Developer\u0026rsquo;\u0026rsquo;s chat interface, provide details about the issue, and request a more thorough explanation. Amazon Q Developer can then dig deeper into the codebase, explain the issue in detail, and guide you on how to remediate it appropriately. This collaborative approach empowers developers to make informed decisions and gain broader knowledge, rather than simply executing a suggestion.\nFigure 6 Amazon Q Developer\u0026rsquo;\u0026rsquo;s chat interface explaining the XSS vulnerability and its security implications through natural language dialogue.\nNote that Amazon Q Developer provides links to documentation and other resources for further reading. Additionally, you can continue the discussion to learn more. For example, imagine you want to understand real-world breaches that have occurred due to the issues Snyk identified. Q provides several examples for me to learn more.\nFigure 7 Amazon Q Developer discussing notable real-world XSS breach examples and their security impacts.\nBeyond fixing issues, Amazon Q Developer can also assist with other development tasks identified by Snyk, such as updating dependencies, refactoring code, or optimizing cloud infrastructure. By integrating these two tools, developers can streamline security scanning, issue investigation, and remediation, thereby significantly increasing their overall productivity.\nConclusion In this blog post, we\u0026rsquo;\u0026rsquo;ve explored how Snyk and Amazon Q Developer are a powerful duo in the modern developer\u0026rsquo;\u0026rsquo;s toolkit. Integrating Snyk\u0026rsquo;\u0026rsquo;s leading security insights with Amazon Q Developer\u0026rsquo;\u0026rsquo;s generative AI capabilities empowers developers to identify, understand, and resolve security vulnerabilities more effectively. This combination allows developers to enhance their skills and improve their capabilities as they tackle security issues. Get started by installing Amazon Q Developer in your IDE and the Snyk plugin.\nSnyk AWS Partner Spotlight Snyk empowers developers worldwide to build secure applications and equips security teams to meet the demands of the digital world. Used by 1,200 customers globally, Snyk\u0026rsquo;\u0026rsquo;s Developer Security Platform automatically integrates with developer workflows and is purpose-built for security teams to collaborate with their development teams.\nAbout the Authors: Omar Faruk\nOmar Faruk is a DevOps Partner Solutions Architect at Amazon Web Services. He helps DevSecOps partners design, build, and operate their workloads and those of mutual customers in AWS. He is passionate about CI/CD, Infrastructure as Code, and next-generation developer experience. Outside of work, he enjoys spending time with family and traveling.\nDavid Schott\nDavid is a seasoned DevSecOps Solutions Engineer with over 15 years of experience helping Fortune 100 companies optimize their security and software delivery efficiency. After driving DevOps adoption and CI development at CloudBees, he now focuses on DevSecOps at Snyk, where he collaborates with strategic partners to enable secure innovation at scale.\n"},{"uri":"https://thanhtai15.github.io/Inter_report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Introducing Quick Start Experience for Amazon Q Developer Pro By Matt Laver | April 7, 2025 | in Amazon Q, Amazon Q Developer, Launch\nFor software developers, development teams, and IT professionals, the Pro tier of Amazon Q Developer is recommended. The pro tier provides higher limits, enterprise-grade administration, analytics dashboards, code customizations, and IP indemnification. Additionally, the Amazon Q Developer Pro tier requires AWS IAM Identity Center. For most production deployments, an organization instance of IAM Identity Center is recommended. An organization instance supports identity-aware sessions which are required to use the pro tier in the AWS Console, in addition to the benefits outlined in the IAM Identity Center user guide.\nHowever, what if your team wants to get started quickly to try out the Amazon Q Developer Pro Tier features in their IDE but there are barriers slowing you down, such as:\nYou don\u0026rsquo;\u0026rsquo;t have plans to adopt IAM Identity Center across your entire organization.\nYou have an organization instance of IAM Identity Center, but you want to deploy Amazon Q Developer Pro for a separate group of users, different from the users in your organization instance.\nYou don\u0026rsquo;\u0026rsquo;t control the AWS Organization you\u0026rsquo;\u0026rsquo;re operating in. For example, a third party controls the AWS organization that manages your AWS accounts and needs time to approve changes.\nTo address these situations, we recently announced a new, simplified two-step setup experience that makes it easier for teams looking to try out the features of Amazon Q Developer Pro in their Integrated Development Environment (IDE).\nManagement Account and AWS IAM Identity Center Before we dive into the quick start setup tour, let\u0026rsquo;\u0026rsquo;s take a step back and consider AWS\u0026rsquo;\u0026rsquo;s recommendation to use a multi-account setup to organize your workloads. You can see the benefits in the AWS whitepaper on Organizing Your AWS Environment Using Multiple Accounts and the recommended approach is to implement this using AWS Organizations. The management account is the AWS account you use to create your organization and this account should be kept minimal and secure, hosting only essential administrative tools like AWS Organizations setup and deploying single sign-on through IAM Identity Center.\nIAM Identity Center is core to the Amazon Q Developer Pro setup. IAM Identity Center users can access AWS accounts and applications using their existing organizational credentials, without having to create and manage separate AWS accounts and passwords.\nFor example, IAM Identity Center can connect and automatically provision users from standards-based identity providers, including Microsoft Active Directory, Okta, Microsoft Entra ID, Google Workspace, or another supported identity provider (IdP) through Security Assertion Markup Language (SAML) 2.0 or OIDC.\nFigure 1 Amazon Q Developer Access Management through AWS IAM Identity Center.\nThis is a great experience for developers as they only need to authorize their Q Developer session using the normal sign-in process through the Identity source already available in their enterprise. Administrators benefit from features like centralized access management, streamlined permission management, and advanced administrator capabilities to view Amazon Q user activity.\nSimplified Setup Experience for Amazon Q Developer Pro The above approach to setting up IAM Identity Center in a management account is ideal but not always feasible, so we created a new getting started experience that makes it easier for teams looking to try out the features of Amazon Q Developer Pro in their Integrated Development Environment (IDE), starting with:\nA standalone account that is not part of an organization managed by AWS Organizations.\nA member account, not the management account, that is part of an AWS Organization but does not have organization-level users managed in IAM Identity Center.\nFor both standalone accounts and member accounts, there is a two-step process. The first step to set up Amazon Q Developer Pro starts by navigating to the Amazon Q console and selecting Get started:\nFigure 2 Amazon Q Console Get started with Amazon Q.\nThe setup will guide you through creating your first user and activating the Amazon Q Developer Pro subscription in your account, this initial step also includes creating an account instance of IAM Identity Center and an AWS-managed Amazon Q Developer application instance. Detailed instructions are available in our documentation Subscribing users to Amazon Q Developer Pro.\nWhen complete, the first user can be seen in the Amazon Q subscription section:\nFigure 3 Amazon Q Subscription.\nThe second step is to subscribe additional team members to the account instance of IAM Identity Center and then subscribe them to Amazon Q Developer Pro through the Amazon Q console.\nFigure 4 IAM Identity Center Users.\nOnce users have successfully subscribed, they will receive an email with instructions on how to activate their Amazon Q Developer Pro subscription and start using the features.\nNote that account instances of IAM Identity Center have limitations. For example, account instances do not support console access. (Users can still use Amazon Q in the console, they will just be subject to the monthly limits of the Free tier.) If you want to use Amazon Q Developer Pro in the console and other AWS sites, you must be a user in an organization instance of IAM Identity Center, in a management account.\nAt this point, it\u0026rsquo;\u0026rsquo;s worth noting that IAM Identity Center can now be configured to change its identity source to a Federated Identity Provider (IdP), see our documentation pages on how to change your identity source.\nCleanup To clean up the resources created in this blog post, first delete the Amazon Q Developer Pro users by following our instructions:\nUnsubscribe users from Amazon Q Developer Pro\nSecond, delete the IAM Identity Center instance:\nDelete your IAM Identity Center instance\nConclusion Getting started with Amazon Q Developer Pro is now even easier with the new, simplified setup experience, you can experience pro features in your IDE, such as higher limits for advanced features like:\nChat, debug code, add tests, and more in your integrated development environment (IDE).\nSpeed up tasks with Amazon Q Developer agents for software development.\nUpgrade applications in a fraction of the time with Amazon Q Developer agent for code transformation.\nGet started with the Amazon Q Developer User Guide on Subscribing users to Amazon Q Developer Pro.\nRead more about how the community is using Amazon Q to code and build applications faster and easier with Amazon Q on community.aws and explore what we\u0026rsquo;\u0026rsquo;re building with Amazon Q Developer here.\nNote that while this post focuses on Amazon Q Developer Pro, developers can still get started at no cost and without an AWS account; Amazon Q Developer offers a perpetual Free tier with monthly limits available to users, see our user guide on the Amazon Q Developer Free tier.\nAbout the Author: Matt Laver\nMatt Laver is a Senior Specialist Solutions Architect at Amazon Web Services (AWS) with a focus on Developer Experience. He is passionate about helping customers find simple solutions to difficult problems.\n"},{"uri":"https://thanhtai15.github.io/Inter_report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: AWS AI/ML and Generative AI Workshop I. EVENT INFORMATION AND AGENDA Time: Saturday, November 15, 2025, from 8:30 AM to 12:00 PM\nLocation: AWS Vietnam Office\nA. Program Summary The program focused on introducing AWS machine learning and generative AI services:\n1. AWS AI/ML Services Overview (9:00 10:30 AM)\nDetailed introduction to Amazon SageMaker - AWS end-to-end Machine Learning (ML) platform Content includes: Data preparation and labeling Model training and fine-tuning Model deployment Integrated MLOps capabilities Live Demo: SageMaker Studio 2. Generative AI with Amazon Bedrock (10:45 AM 12:00 PM)\nFocus on using Amazon Bedrock to deploy Generative AI Key topics: Comparing and selecting Foundation Models (Claude, Llama, Titan) Prompt Engineering techniques Chain-of-Thought reasoning Few-shot learning Retrieval-Augmented Generation (RAG) architecture Knowledge Base integration Bedrock Agents for multi-step workflows Guardrails for safety and content filtering II. AI-DRIVEN DEVELOPMENT LIFECYCLE (AIDLC) METHODOLOGY AIDLC (AI-Driven Development) is a methodology and approach aimed at accelerating software development, with the ability to reduce working time from two weeks to approximately 1.5 days.\nA. Human-Centric Philosophy No Autonomous Decisions:\nThe core philosophy of AIDLC is to not allow AI to make any decisions replacing humans Human Role:\nResponsibilities of developers and stakeholders: Validation Decisioning Oversight If AI raises questions to clarify requirements, humans must answer for AI to proceed Mandatory Plan Creation:\nWhen requesting AI to perform any task, it must create a plan first Benefits: Helps users know exactly what AI wants to do Avoids context overload If the plan is inaccurate, request AI to create a new plan Iteration is a crucial part of the AIDLC methodology B. Process and Context Management 1. AIDLC Phases (Faces):\nInception: Initial phase\nDefine requirements Break down into User Stories Group User Stories into Units (Unit of Work) Construction: Building phase\nImplement based on grouped Units Operation: Operating phase\nDeploy CI/CD 2. Context Management:\nLanguage Priority:\nAI understands language context better than code Instead of providing entire source code as input, extract information into: Project summary Technical architecture Domain model This helps AI understand better and avoids context overload Use Separate Sessions:\nCreate separate work sessions for each task (like each Unit) Better context control Minimize hallucination risks from previous task contexts 3. Mob Development:\nAIDLC proposes Mob Development method Multiple roles working together on the same computer: BA (Business Analyst) Engineer Solution Architect QA (Quality Assurance) Benefits: Everyone validates output immediately Increases speed and work efficiency III. AI-INTEGRATED TOOL: KIRO IDE Kiro is AWS Integrated Development Environment (IDE), built to integrate AI from the start, similar to Coder or VS Code.\nCurrently uses Large Language Models (LLMs) like Claude (e.g., Claude 3.4, 3.7, 4.5) A. Spec-Driven Development (SDD) Core Feature:\nSDD is a built-in feature in Kiro Users don\u0026rsquo;t start with code but by creating spec documents Default Spec Files:\nKiro creates three basic files in the .kiro folder: requirement design task list Limitations:\nSDD is evaluated as rigid and difficult to scale for large projects More suitable for: Small projects Creating prototypes Due to fewer documents, allows AI more \u0026ldquo;space\u0026rdquo; for errors compared to detailed AIDLC process B. AIDLC Integration in Kiro Kiro supports applying custom methodologies through Advanced Context Management Implementation: Define AIDLC workflow in steering file Place in .kiro/steering folder Kiro will automatically run phases (faces) according to AIDLC process C. Agent Hooks Kiro allows creating Agent Hooks, similar to webhooks but for AI Agents Functions: Define rules using natural language Example: \u0026ldquo;whenever JS file is saved, run unit test\u0026rdquo; Automate tasks based on events Allow main AI Agent to focus on core logic IV. LESSONS LEARNED AND RECOMMENDATIONS 1. Prompting Mindset When requesting AI, clearly define AI\u0026rsquo;s role (persona) Input and Output must be clear Output should be recorded in a specific file (like Markdown) rather than just stored in temporary memory 2. Avoid Prohibitions Instead of: \u0026ldquo;don\u0026rsquo;t do this\u0026rdquo; (don\u0026rsquo;t implement this) Say: \u0026ldquo;implement this\u0026rdquo; Reason: High probability that AI will do prohibited things 3. Importance of Developers Although AI supports rapid development, developer value still lies in: Monitoring capability Validation Decision making Time spent on validation and oversight is \u0026ldquo;well-deserved time\u0026rdquo; V. ILLUSTRATIVE EXAMPLE Using AIDLC and Kiro in software development is like controlling a self-driving car (AI) on a complex route:\nYou (the developer) are the one who:\nProvides detailed map (Plan) Identifies important stops (Units) Continuously checks if the car is going in the right direction (Validation) You don\u0026rsquo;t need to drive every meter, but you are fully responsible for every decision on the road to ensure safe and efficient arrival at the destination.\nVI. CONCLUSION AND PERSONAL ASSESSMENT The AWS AI/ML and Generative AI Workshop event provided extremely valuable knowledge about:\nHow to responsibly apply AI to software development processes AIDLC methodology helps accelerate development while ensuring quality Kiro IDE tool with powerful AI integration capabilities The most notable point is the Human-Centric philosophy - emphasizing that AI is a support tool, not a human replacement. This helps me better understand the role and responsibility of developers in the AI era.\nSome event photos Add your event photos here\n"},{"uri":"https://thanhtai15.github.io/Inter_report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Thanh Tai\nPhone Number: 0968963693\nEmail: taintse183560@fpt.edu.vn\nUniversity: FPT Ho Chi Minh City University\nMajor: Information Technology\nClass: SE183560\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://thanhtai15.github.io/Inter_report/5-workshop/5.4-databaseandstorage/5.4.2-create-s3/","title":"Create an AWS S3","tags":[],"description":"","content":"In this section you will create S3 bucket to storage photos\nOpen the Amazon S3 Click Create bucket In create console, fill in name of bucket Then leave everything as default like picture Click Create bucket bottom "},{"uri":"https://thanhtai15.github.io/Inter_report/5-workshop/5.3-vpc/5.3.3-create-security-groups/","title":"Create Security Groups","tags":[],"description":"","content":" Open the Amazon VPC console Choose Security Groups -\u0026gt; click Create security group In Create Security group Spacific name of Security group Choose VPC created Add rule Inbound and Outbound for Security Group In ReGenZet project we have 4 Security groups, which are fargate-sg, rds-sg, alb-sg and endpoint-sg. fargate-sg is security group for AWS ECS Fargate Do again step 1 -\u0026gt; 3 Choose fargate-sg created Inbount Add rule security group of Application Load Balancer (ALB) is alb-sg Outbound Add rule security group of MySQl (rds-sg) and HTTPS Click Create security group rds-sg is security group for AWS RDS Do again step 1 -\u0026gt; 3 Choose rds-sg created Inbound Add rule security group of MySQL like this instruct Outbound is not Add rule Click Create security group alb-sg is security group for AWS Application Load Balancer (ALB) Do again step 1 -\u0026gt; 3 Inbound Add rule HTTPS and HTTP type like this instruct Outbound Add rule security group of AWS ECS Fargate (fargate-sg) Click Create security group endpoint-sg is security group for VPC Endpoints Do again step 1 -\u0026gt; 3 Inbound Add rule security group of AWS ECS Fargate (fargate-sg) Outbound is not Add rule "},{"uri":"https://thanhtai15.github.io/Inter_report/5-workshop/5.3-vpc/5.3.2-create-subnets/","title":"Create Subnets","tags":[],"description":"","content":"Create Public Subnet Open the Amazon VPC console Choose Subnets -\u0026gt; click Create subnet In Create subnet console: Choose VPC created Fill subnet name Choose AZ Spacific IPv4 subnet Then click Create subnet Create Private Subnet Do again step 1 -\u0026gt; 4 Click this Subnet -\u0026gt; Choose Route table Choose Route table ID is private Click Save "},{"uri":"https://thanhtai15.github.io/Inter_report/5-workshop/5.3-vpc/5.3.4-create-vpc-endpoints/","title":"Create VPC Endpoints","tags":[],"description":"","content":" Open the Amazon VPC console Choose Endpoints -\u0026gt; click Create endpoints In Create console: Fill name of VPC endpoint Type is AWS Services Search In this project we have 5 VPC Endpoints VPC Enpoint S3 Gateway (apexev-s3-gateway) In search box -\u0026gt; com.amazonaws.ap-southeast-1.s3 Choose type Gateway Choose VPC created Choose Route table private Policy is Full access Click Create endpoint VPC Enpoint ECR API \u0026amp; DKR Interface In search box -\u0026gt; ecr Will see ecr.api and ecr.dkr interface Do this twice and choose a different one each time Choose VPC created Tick two subnet private in two difference AZ Choose endpoint-sg Click Create endpoint VPC Enpoint Logs In search box -\u0026gt; com.amazonaws.ap-southeast-1.logs Choose com.amazonaws.ap-southeast-1.logs Choose VPC created Tick two subnet private in two difference AZ Choose endpoint-sg Click Create endpoint VPC Enpoint AWS SNS In search box -\u0026gt; com.amazonaws.ap-southeast-1.sns Choose com.amazonaws.ap-southeast-1.sns Choose VPC created Tick two subnet private in two difference AZ Choose endpoint-sg Click Create endpoint "},{"uri":"https://thanhtai15.github.io/Inter_report/5-workshop/5.2-project-architecture/","title":"Project Architecture","tags":[],"description":"","content":"Architecture Overview This document describes the recommended production-ready architecture for ReGenZet — an EV Garage Management System — and explains the technology choices used in the workshop.\nFrontend: A React single-page application deployed and hosted via AWS Amplify. Amplify provides automated CI/CD, asset hosting and integration with CloudFront for global caching and fast client delivery. Backend: Containerized Spring Boot services packaged as Docker images and deployed to Amazon ECS (Fargate). Fargate removes host management overhead and provides scalable, serverless compute for microservices. Database: Amazon RDS (MySQL/Postgres) hosted in private subnets. RDS provides automated backups, snapshots, Multi-AZ options and encryption at rest (KMS). API Management: Amazon API Gateway exposes a single HTTPS entry point for client traffic, handles request routing, TLS termination and request throttling. Media Storage: Amazon S3 holds vehicle inspection images, videos and other media. Use presigned URLs for secure direct-upload/download to offload traffic from the backend. Asynchronous / Serverless components: Email pipeline: Backend (Spring) publishes an event to SNS, which triggers a Lambda to send email via Amazon SES. AI/Chat pipeline: Frontend → API Gateway → Lambda → Amazon Bedrock (or other managed LLM) for inference and conversational workflows. Network \u0026amp; Security: Deploy resources inside a VPC with well-defined Public and Private subnets. Use Security Groups and NACLs to control traffic. Use VPC Endpoints (Gateway and Interface) for S3 and private service access, keeping traffic inside the AWS network. Why this architecture? Security-first\nThe backend and RDS instances reside in private subnets and never expose database ports to the public internet. API Gateway and load-balanced frontends terminate TLS at the edge, while internal services communicate over private networking. Cost-efficiency\nFargate and Lambda offer a pay-for-what-you-use model. When appropriate, consider Fargate Spot for non-critical workloads and configure autoscaling and lifecycle policies for S3 to reduce costs. Operational simplicity \u0026amp; modern patterns\nClear separation of concerns between frontend and backend, with API Gateway as a single ingress point. Event-driven components (SNS, Lambda) decouple email and AI processing from request-response paths, improving resilience and scalability. Developer productivity\nAWS Amplify simplifies frontend CI/CD and hosting. Container workflows with Docker + ECR and ECS Fargate enable reproducible deployments for backend services. Security and best-practice highlights\nLeast-privilege IAM roles for services and cross-account access where needed. KMS-managed encryption for RDS and S3 objects. WAF and rate-limiting on API Gateway to mitigate application-level attacks. This architecture balances enterprise-grade security with cost-effective serverless patterns and provides a pragmatic path for incremental adoption of advanced features (observability, multi-region DR, Bedrock-powered AI).\n"},{"uri":"https://thanhtai15.github.io/Inter_report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"APEX-EV Electric Vehicle Service Platform 1. Executive Summary RenGen is a comprehensive management platform designed to digitize and optimize maintenance workflows at service centers. The system centrally manages the entire service lifecycle—from request intake and repair processing to customer care—helping to eliminate manual tasks and enhance efficiency. Leveraging the power of the AWS cloud, RenGen combines flexible container architecture on Amazon ECS Fargate with the intelligent processing capabilities of Generative AI through Amazon Bedrock. The solution integrates automated development processes (CI/CD) from GitLab, ensuring rapid deployment speeds, high security, and rigorous monitoring, delivering a superior experience for end-users.\n2. Problem Statement What’s the Problem? Current operational processes rely heavily on manual methods, leading to inefficiencies, fragmented data, and a lack of intelligent support tools for automated customer interaction.\nThe Solution The platform employs a modern architecture, starting at the Edge layer with Amazon Route 53 for user routing. The interface (Frontend) is hosted on AWS Amplify Hosting, ensuring fast and stable access. Amazon API Gateway acts as the central hub, intelligently routing requests.\nTo ensure security, critical components such as ECS Fargate and the Amazon RDS database are placed in a Private Subnet, completely isolated from the public internet. Image data is stored on Amazon S3 and accessed securely via S3 Endpoints. Additionally, the software development process is fully automated: source code from GitLab is packaged and pushed to Amazon ECR for deployment to ECS.\nBenefits and Return on Investment Adopting this architecture delivers a significant competitive advantage by integrating Artificial Intelligence (GenAI) via Amazon Bedrock, which helps automate customer care and data analysis. The system ensures high availability and data security thanks to the VPC network separation design (Public/Private subnets).\nThe CI/CD process integrated with GitLab and ECR helps minimize downtime when updating new features, while Amazon CloudWatch provides comprehensive monitoring to detect incidents instantly. The cost model is optimized thanks to the use of Fargate (Serverless container) and Lambda (Pay-per-use), ensuring businesses only pay for the actual resources used. This investment not only resolves current operational challenges but also creates a solid technological foundation for long-term growth, with the expected Return on Investment (ROI) period significantly shortened.\n3. Solution Architecture The RenGen management platform utilizes a modern architecture deployed on AWS (Region ap-southeast-2), initiated by user access via Amazon Route 53 at the Edge layer. The User Interface (Frontend) is hosted on AWS Amplify Hosting, which establishes a direct connection to Amazon API Gateway as the central entry point.\nFrom the API Gateway, the data flow is strategically divided into three distinct paths:\nAI Tasks: Requests are routed to AWS Lambda to interact with Amazon Bedrock for generative AI capabilities. Notification Tasks: Asynchronous requests trigger AWS Lambda to handle email communications via Amazon SES. Core Business Logic: Traffic is directed through an Application Load Balancer (ALB) located in the Public Subnet, then forwarded to Amazon ECS Fargate instances secured within a Private Subnet. Data \u0026amp; Security:\nRelational data is persistently stored in Amazon RDS within the Private Subnet. To optimize security and performance, the architecture utilizes VPC Endpoints to keep traffic strictly within the AWS internal network:\nStatic assets and images stored in Amazon S3 are accessed securely via S3 Endpoints. Container images are pulled directly from Amazon ECR via ECR Endpoints. By leveraging these endpoints, the system eliminates the need for a NAT Gateway, thereby reducing costs and minimizing public internet exposure.\nDevOps \u0026amp; Monitoring:\nGitLab is used for source code management and CI/CD, automatically pushing deployments to Amplify (Frontend) and container images to ECR (Backend). AWS Services Used Route 53: DNS service, responsible for routing the domain (Edge layer) to the application. AWS Amplify Hosting: Hosts the web interface (frontend) and can integrate with CDN/WAF. In the diagram, it receives traffic from Route 53. Amazon API Gateway: The main entry point (Gateway), receiving and routing all requests from the frontend/Amplify to processing services. AWS Lambda (Bedrock): Handles AI/Generative AI tasks (prediction/content generation) by communicating with Amazon Bedrock. AWS Lambda (SES): Handles asynchronous tasks, such as processing notifications to send emails via AWS SES. Amazon Bedrock: General AI service (Gen AI), providing foundation models to execute intelligent business operations. AWS SES: Email sending service, performs the sending of notifications, quotes, or processing results from Lambda. VPC: Virtual network environment containing and protecting AWS resources (like ALB, ECS Fargate, RDS). ALB (Application Load Balancer): Load balancer, distributing traffic from API Gateway to application containers running on ECS Fargate. Amazon ECS Fargate: Runs the backend application as containers without server management, handling core business logic. Amazon RDS: Provides a relational database, placed in a Private Subnet to store structured data. Amazon S3: Stores multimedia files like photos or other large data. ECR: Repository for application container images (Docker), used by ECS Fargate for deployment. AWS CloudWatch: Monitoring service, collecting logs and metrics from the entire system to track performance and detect issues. Component Design Request Handling: Amazon Route 53 routes user domain requests to AWS Amplify Hosting, where the frontend interface is hosted. From there, API requests are forwarded to Amazon API Gateway, which acts as the central entry point to receive and route all incoming traffic.\nBusiness Logic Processing:\nCore Logic: All primary business operations are handled by containerized applications running on Amazon ECS Fargate, deployed within a Private Subnet to ensure maximum security. AI \u0026amp; Asynchronous Tasks: Generative AI tasks are processed by AWS Lambda interacting with Amazon Bedrock. Auxiliary tasks, such as email notifications, are handled by separate AWS Lambda functions triggering Amazon SES. Network Infrastructure:\nPublic Subnet: Hosts the Application Load Balancer (ALB) to receive and distribute external traffic. Private Subnet: Dedicated to sensitive resources including ECS Fargate and Amazon RDS, ensuring they are isolated from direct public internet access. VPC Endpoints: The system explicitly utilizes S3 Endpoints and ECR Endpoints. This design allows ECS Fargate to pull container images and access file storage securely within the AWS internal network, without traversing the public internet. Data Storage:\nAmazon RDS: Stores sensitive, structured relational data. Amazon S3: Stores multimedia files and large datasets. Deployment and Monitoring: The deployment pipeline is managed via GitLab, which triggers updates to AWS Amplify (Frontend) and pushes Docker images to Amazon ECR (Backend). Amazon CloudWatch provides comprehensive monitoring of performance logs and metrics across all services, from ECS and Lambda to RDS.\n4. Technical Implementation Implementation Phases The development project for the RenGen Smart Electric Vehicle Maintenance Platform — including the integration of an AI virtual assistant and a service management system — undergoes 4 phases:\nResearch and Architectural Design: Research suitable technologies (React.js, Spring Boot, AWS Bedrock) and design a system architecture combining Containers (ECS) and Serverless (Lambda) on AWS (1 month prior to commencement). Cost Estimation and Feasibility Check: Use the AWS Pricing Calculator to estimate operating costs for core services such as ECS Fargate, RDS, and token costs for Amazon Bedrock, and propose the most feasible solution. Architecture Adjustment for Cost/Solution Optimization: Refine the architecture, select appropriate configurations for ECS Fargate and RDS, and optimize Lambda runtime (timeouts) to balance AI processing performance and cost. Development, Testing, and Deployment: Program the React.js application (Frontend) and Spring Boot (Backend), integrate the Bedrock Agent, deploy CI/CD pipelines via GitLab, package Docker images to ECR, and launch operations on ECS. Technical Requirements Technical Requirements User Interface (Frontend): Practical knowledge of React.js to build scheduling interfaces and chat with the AI virtual assistant. Use AWS Amplify to automate the deployment process (Hosting), connect with Amazon API Gateway to send secure processing requests, ensuring a smooth user experience on all devices. Core System (Backend \u0026amp; Infrastructure): In-depth knowledge of Java/Spring Boot to develop maintenance business logic. The application is packaged using Docker, with images stored on AWS ECR and running on Amazon ECS Fargate. Requires understanding of Amazon RDS for relational databases (storing vehicle profiles, maintenance history). Specifically, requires AWS Lambda (Python) programming skills to connect with Amazon Bedrock (AI/Chatbot processing) and AWS SES (sending asynchronous email notifications). Manage detailed user authentication and authorization (customers/technicians) via Amazon Cognito. 5. Timeline \u0026amp; Milestones Project Timeline\nPhase 1 (Week 1-2): Design and Foundation:: Analyze \u0026amp; Design detailed AWS architecture (VPC, Subnets, Security Groups). Design Database (RDS Schema) and define APIs (Swagger/OpenAPI). Configure infrastructure environment: Setup VPC (Public/Private Subnets), IAM Roles, and Amazon Cognito (User Pools). Setup CI/CD: Configure Pipeline on GitLab to automatically build Docker Images, push to Amazon ECR, and deploy Frontend to AWS Amplify. Phase 2 (Week 3-4): Core Service Flow Development:: Develop Customer flow (Frontend/Backend): Registration/Login, Vehicle Profile Management, Appointment Scheduling (stored in RDS). Develop Service Advisor flow: Vehicle Reception, Create Quotations and Repair Orders. Phát triển luồng Kỹ thuật viên: Xem danh sách việc cần làm (Task list), Cập nhật tiến độ bảo dưỡng và tải ảnh/video lên Amazon S3. Phase 3 (Week 5-6): Administration \u0026amp; Advanced Feature Development:: Build Administration Module: Report Dashboard, Spare Parts Management (Inventory), and Personnel Management.Build Administration Module: Report Dashboard, Spare Parts Management (Inventory), and Personnel Management. Write AWS Lambda to connect Amazon Bedrock Agent (AI Chatbot for customer support) and expose via API Gateway. Write AWS Lambda to trigger AWS SES for sending automatic notification emails/quotations to customers. Configure NAT Gateway so resources in Private Subnet (Lambda, ECS) can securely connect to the Internet/AWS Services. Phase 4 (Week 7-8): Testing, Optimization, and Operation:: Internal User Acceptance Testing (UAT) to ensure the flow from Web -\u0026gt; API Gateway -\u0026gt; Lambda/ECS -\u0026gt; DB operates smoothly. Optimize security: Configure AWS WAF (block SQL Injection, XSS) and review IAM access rights. Operational monitoring: Setup Dashboard on Amazon CloudWatch to track logs and metrics of ECS Fargate and Lambda. Official deployment. 6. Budget Estimation Infrastructure Costs\nAmazon ECS Fargate: ~11.00 USD/month. Application Load Balancer (ALB): ~16.43 USD/month. Amazon Bedrock (AI): ~5.00 USD/month (Calculated by Token count). AWS Lambda: 0.00 USD/month (Free Tier). Amazon RDS \u0026amp; ElastiCache: 0.00 USD/month (Free Tier). S3 Standard: ~0.15 USD/month. AWS Amplify \u0026amp; API Gateway: ~0.50 USD/month. Amazon CloudWatch: 0.00 USD/month (Free Tier). Amazon SES: 0.00 USD/month (Free Tier). Total: ~32.63/month.\n7. Risk Assessment Risk Matrix System downtime: High impact, low probability. Security breach/Data loss: Very high impact, low probability. Operational cost overrun: Medium impact, medium probability. Mitigation Strategies System: Deploy infrastructure across Multi-AZ for RDS and ECS Fargate. Use Application Load Balancer for automatic load distribution and recovery. Security: Use AWS WAF to filter malicious requests. Strict authorization with Amazon Cognito and apply least privilege principle. Backend placed in a separate network (Private Subnet). Cost: Use AWS Budgets to set alerts when costs exceed thresholds. Regularly monitor and optimize resources (right-sizing) to avoid waste. Incorrect AI response: Medium impact, medium probability. Contingency Plans System: Deploy ECS Fargate and RDS infrastructure across Multi-AZ to ensure high availability. Use Application Load Balancer for automatic load coordination and Auto-scaling to expand Tasks when traffic spikes. AI Quality: Limit Bedrock Agent response scope via strict Prompt Engineering (System Prompts) and only allow information retrieval from moderated Knowledge Bases. Enable Automated Backups for RDS and Point-in-time Recovery to restore data to any point in time. 8. Expected Outcomes Technical Improvements: Technical Improvements: Successfully build a modern Hybrid Architecture combining Microservices (ECS Fargate) and Serverless (Lambda, Bedrock), ensuring flexible scalability without managing physical servers.\nLong-term Value Enhance customer experience: AI virtual assistant operating 24/7 helps reduce waiting time, increasing appointment conversion rates and car owner satisfaction.\nData assets: Maintenance history and interaction behavior data are centrally stored on RDS/S3, creating a premise for deploying AI Predictive Maintenance models for electric vehicle batteries and motors in the future.\n"},{"uri":"https://thanhtai15.github.io/Inter_report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"GenAI-powered App-DB Modernization Workshop \u0026amp; DevOps on AWS Workshop Takeaway 1. PURPOSE OF THE EVENT This combined event aims to share DevOps culture and principles along with best practices on the AWS platform. Additionally, the workshop provides an opportunity for learners to access Cloud and AI platforms.\nContext and Importance:\nLegacy physical machines, virtual machines, and old servers in enterprises cannot support AI technology chips Businesses are focusing on investing in Cloud to support AI technology Opens up great opportunities for young people in Cloud and AI fields The workshop aims to equip knowledge about:\nCore DevOps performance metrics:\nDORA (DevOps Research and Assessment) MTTR (Mean Time to Resolution) Deployment Frequency Deploying modern technologies on AWS:\nContinuous Integration/Continuous Deployment (CI/CD) Infrastructure as Code (IaC) Monitoring \u0026amp; Observability Building a solid foundation:\nAI and Cloud Leveraging great opportunities in the future 2. SPEAKER LIST Main speakers sharing content include:\nLam Thu Kiet - Senior DevOps Engineer at FPT Software\nShared experience in banking domain Practical chatbot deployment Shared about DevOps processes in enterprise environment Danh Hoang Hieu Nghi - AI Engineer at Renob (AWS partner company)\nShared experience deploying real-world AI applications Guided best practices in AI deployment Linh Lan Hoang Anh - Senior student\nShared perspectives on the latest AWS AI services Especially services beyond Bedrock/SageMaker 3. HIGHLIGHTED CONTENT The workshop\u0026rsquo;s highlighted content includes main parts about DevOps on AWS and core AI techniques.\nA. DevOps Mindset and Principles DevOps Spirit:\nSharing DevOps culture and principles Benefits and key metrics: DORA (DevOps Research and Assessment) MTTR (Mean Time to Resolution) Deployment Frequency Change Failure Rate B. CI/CD Pipeline on AWS 1. Source Control:\nUsing AWS CodeCommit Git strategies: GitFlow Trunk-based development 2. Build \u0026amp; Test:\nConfiguring CodeBuild Automated testing Build optimization 3. Deployment:\nUsing CodeDeploy with deployment strategies: Blue/Green: Parallel deployment, fast switching Canary: Gradual deployment, risk reduction Rolling updates: Sequential updates 4. Orchestration:\nPipeline automation with CodePipeline Workflow automation Multi-stage deployment C. Infrastructure as Code (IaC) AWS CloudFormation:\nTemplate-based infrastructure Stack management AWS CDK (Cloud Development Kit):\nMulti-language support (TypeScript, Python, Java, C#) Reusable patterns Type-safe infrastructure D. Monitoring \u0026amp; Observability CloudWatch:\nMetrics collection Logs aggregation Alarms configuration Dashboards visualization AWS X-Ray:\nDistributed tracing analysis Performance bottleneck detection Full-stack observability E. Foundation Models (FM) Foundation Models Characteristics:\nTrained on very large datasets with billions of parameters Uses self-supervised learning Differences from traditional ML: Traditional ML: Only handles one specific task FM: Can handle broader tasks and generalize multiple tasks with just one prompt F. Prompting Techniques 1. Zero-shot:\nBasic instruction only No examples needed 2. Few-shot:\nProvides context and examples Refines model output 3. Chain of Thought (CoT):\nFoundation technique helping LLM break down questions Step-by-step reasoning for more accurate results Foundation for AI Agent operation G. Retrieval-Augmented Generation (RAG) RAG - Model adopted by many companies\nThree operational steps:\nRetrieval: Retrieve internal information from knowledge space Argumentation: Combine information with user question Generation: Generate answer Embedding Technology:\nConverts text to N-dimensional vector space Helps machine learning understand sentences Example: Amazon Titan Embedding H. Amazon Bedrock Agent Core Essential framework for deploying and scaling AI applications\nSolves complex problems:\nMemory Management:\nHelps AI remember user context Maintains context across multiple sessions Identity:\nAuthorization User management Tool Calling:\nAutomates actions Calls internal APIs Uses browser tool to interact with web browsers I. AWS Pre-trained AI Services Pre-trained AI Services - Accessible via API only:\n1. Amazon Recognition\nComputer Vision 2. Amazon Translate\nNatural language translation Different from query-based translation 3. Amazon Transcribe\nSpeech-to-text conversion Supports streaming Multi-speaker recognition 4. Amazon Polly\nText-to-speech conversion Text-to-speech for call centers 5. Amazon Comprehend\nNatural Language Processing (NLP) Sentiment analysis Sensitive information detection 6. Amazon Kendra\nEnterprise search Supports semantic search Supports RAG in internal documents 4. LESSONS LEARNED Comprehensive DevOps Process Mastered components of CI/CD pipeline on AWS Understand advanced deployment strategies: Blue/Green deployment Canary deployment Rolling updates Product Evaluation Criteria For a product to be considered \u0026ldquo;passed\u0026rdquo; (to get points): Must be a \u0026ldquo;working product\u0026rdquo; not just on paper Complete approximately 5 to 6 features out of 10 proposed features Example: E-commerce website must: Display products Add to cart Complete payment Infrastructure Modernization Mindset Clearly recognize that businesses are focusing on investing in Cloud Legacy systems don\u0026rsquo;t support AI chips This opens up great opportunities for young people AI Cost Optimization Need to optimize phrasing and refine keywords in prompting Avoid verbosity, reduce input tokens Save computation costs of LLM models AI Reasoning Capability CoT is an important technique helping LLM reason Deliver more accurate results Foundation for complex AI Agent tasks 5. APPLICATION TO WORK A. CI/CD Integration Automated deployment process:\nApply CodePipeline and related services Automate deployment process Use strategies like Blue/Green to minimize downtime Benefits:\nIncrease delivery speed Reduce human error Improve quality assurance B. Incident Management Incident Management:\nApply incident management processes Postmortem reporting Continuous improvement in DevOps environment C. Developing Scalable AI Products Transition from Local to Production:\nUse Bedrock Agent Core Transform AI applications from localhost to multi-user systems Ensure management of: Memory (context retention) Identity (authorization) Tool calling (API integration) D. Building Professional Portfolio Portfolio Development:\nComplete products meeting \u0026ldquo;working\u0026rdquo; standards Include in portfolio and CV Increase internship or job opportunities 6. EVENT EXPERIENCE CI/CD and Observability Demo Watched live demo of complete CI/CD pipeline deployment Full-stack observability setup Understand how components interact Bedrock Agent Core Browser Tool Introduced and saw demo of Bedrock Agent Core usage Browser tool for automating actions on web browsers Witnessed powerful AI automation capabilities Expert Interaction Had opportunities to interact with experts Real-world experience in banking domain Chatbot deployment in production environment Career Roadmap Understand DevOps career paths AWS certification roadmap Career guidance from senior engineers Pre-trained AI Services Understand how to use AWS pre-trained AI services Examples: Translate, Transcribe, Polly, Kendra Solve specific problems without training models from scratch 7. KEY TAKEAWAYS 1. Product Quality in Practice Technology products must be working products Complete proposed core features Not just stopping at design or documentation 2. Prompting Optimization Optimizing phrasing and keywords is very important Reduce token costs Improve response quality from Large Language Models (LLM) 3. RAG as Knowledge Integration Solution RAG model is an effective solution Integrates Foundation Model with enterprise internal knowledge Through Embedding process 4. DevOps Requires Synchronization Success in DevOps is not just using tools Need to apply appropriate culture and principles Use tools like CloudWatch/X-Ray to maintain system observability 8. CONCLUSION AND ASSESSMENT The \u0026ldquo;GenAI-powered App-DB Modernization \u0026amp; DevOps on AWS\u0026rdquo; workshop provided comprehensive insights into:\nModern DevOps processes on AWS Generative AI technology and practical deployment Combination of DevOps and AI to build scalable products Illustrative Comparison Developing AI today is like building a city:\nInitially, you can build a single house (localhost application) But if you want thousands of people to use simultaneously (scale) You cannot just copy that house But need a management system: Traffic (routing) Utilities (resources) Public security (security) Bedrock Agent Core plays the role of this management system Ensures everything runs smoothly and can scale This is an important foundation helping students prepare for:\nLabor market in the AI era Careers in Cloud and DevOps fields Building modern technology products Some event photos Add your event photos here\n"},{"uri":"https://thanhtai15.github.io/Inter_report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"WORKLOG WEEK 2: Infrastructure, Management Tools and Cost Optimization Time: 18/08/2025 22/08/2025\nWeek 2 Objectives: Master concepts about AWS Global Infrastructure (AZ, Region, Edge Location). Understand management tools and basic security mechanisms. Apply cost optimization methods and budget management. Begin supplementary research on AWS Well-Architected Framework (WAF). Tasks to be deployed this week: Day Task Start Date Completion Date Reference Materials Mon Deep dive into AWS Global Infrastructure: Data Center (hardware optimization), Availability Zone (AZ, fault isolation), Region (minimum 3 AZs), and Edge Location (presence points, running CloudFront, WAF). 18/08/2025 18/08/2025 Tue Learn AWS Management Tools: AWS Management Console (Root user, IAM user), AWS CLI (command line), and AWS SDK (automate request processing and authentication). 19/08/2025 19/08/2025 Wed Deep research on Cost Optimization: Payment methods On-Demand (default), Long-term commitment (Reserve Instance/Saving Plan), and Temporary resources (Spot, low cost but can be reclaimed). 20/08/2025 20/08/2025 Thu Practice Lab: Set up AWS Budget to create and monitor cost usage thresholds or commitment usage thresholds. Learn how to use Cost Allocation Tagging to manage costs by department/application. 21/08/2025 21/08/2025 Fri Supplementary research: Learn overview of AWS Well-Architected Framework (WAF) - self-assessment architecture framework. Read about AWS support plans (Basic, Developer, Business, Enterprise) and appropriate usage timing. 22/08/2025 22/08/2025 Week 2 Achievements: Clearly understood and distinguished core Amazon VPC components:\nCIDR block Subnet (Public vs Private) Route Table \u0026amp; Internet Gateway (IGW) Grasped networking connectivity and load distribution solutions:\nVPN \u0026amp; DirectConnect (Hybrid Cloud connectivity) Load Balancer (Application load balancing) Setup and managed network security layers:\nSecurity Group: Configured Instance-level firewall (Stateful) Network ACLs: Configured Subnet-level firewall (Stateless) Proficiently used the VPC Resource Map tool to visualize and control resource traffic flow within the network.\nCompleted the first meeting with the final project team.\n"},{"uri":"https://thanhtai15.github.io/Inter_report/5-workshop/5.4-databaseandstorage/5.4.3-create-ecr/","title":"Create AWS ECR","tags":[],"description":"","content":" Open the Amazon Elastic Container Registry In create console, fill in repository name Choose Mutable in Image tag mutability Then click Create "},{"uri":"https://thanhtai15.github.io/Inter_report/5-workshop/5.3-vpc/","title":"VPC","tags":[],"description":"","content":"Intro VPC (Virtual Private Cloud) is a logically isolated virtual private network space in AWS Cloud. It acts as your personal data center in the cloud, giving you complete control over the network environment.\nCore components: Route Table Subnets Internet Gateway Security Groups Create VPC Open the Amazon VPC console Choose Your VPCs, then click Create VPC In the create VPC console: Specify name of the Name tag: my-vpc-01 IPv4 CIDR : 10.0.0.0/16 Then click Create VPC Content Create Route Table \u0026amp; Internet Gateway Create Subnets Create Security Groups Create VPC Endpoints "},{"uri":"https://thanhtai15.github.io/Inter_report/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Mastering AWS DevOps: CI/CD, IaC, and Containers 1. EVENT INFORMATION Event Name: Mastering AWS DevOps: CI/CD, IaC, and Containers\nTime and Location:\nDate: Monday, November 17, 2025 Time: 8:30 AM 5:00 PM Location: AWS Scope: In-depth workshop on DevOps on the AWS platform, focusing on CI/CD, Infrastructure as Code, and Container technologies.\n2. MAIN CONTENT AND KEY CONCEPTS A. DevOps Mindset and Principles DevOps Definition:\nDevOps is the bridge between Development (Dev) and Operations (Ops) Requires DevOps practitioners to understand both areas thoroughly Solves the problem: Why code runs on developer\u0026rsquo;\u0026rsquo;s machine but fails on server Core Objectives:\nAutomation is the primary focus of DevOps Purpose: Minimize errors caused by human factors Key Metrics:\nDORA (DevOps Research and Assessment) MTTR (Mean Time to Resolution) Deployment Frequency Value of Metrics:\nTracking these metrics creates a comprehensive picture for stakeholders Helps them see the value of DevOps investment B. CI/CD Pipeline on AWS The workshop focuses on AWS core services for Continuous Integration/Continuous Deployment (CI/CD).\n1. Source Control:\nUsing AWS CodeCommit Git strategies: GitFlow Trunk-based development 2. Build \u0026amp; Test:\nConfiguring CodeBuild Building test pipelines Important: Not just building, but continuously testing code Applied to AI: Even AI-generated code requires testing 3. Deployment:\nUsing CodeDeploy with advanced deployment strategies: Blue/Green: Parallel deployment, fast switching Canary: Gradual deployment, risk reduction Rolling updates: Sequential updates 4. Orchestration:\nAutomating the entire process with AWS CodePipeline Distinguishing CI, CD, and Continuous Deployment:\nContinuous Integration (CI):\nDev team workflow process Includes: Source code management Authorization Container building (e.g., Docker file) Automated code scanning for quality and security checks Continuous Delivery (CD):\nAutomated product deployment process Requires manual intervention (approval button click) After passing testing stages Continuous Deployment:\nFully automated deployment process From code commit to production application No manual intervention required C. Infrastructure as Code (IaC) What is IaC:\nUsing code to deploy and manage infrastructure Advantages of IaC:\nRapid Automation:\nCreate hundreds of servers with just one template Consistency:\nEnsures configuration doesn\u0026rsquo;\u0026rsquo;t drift Portable:\nEasy environment transition Just move code to different environment Documentation:\nCode serves as documentation Others can read and understand how infrastructure works Click Ops Problem:\nConfiguring by \u0026ldquo;clicking\u0026rdquo; on console (Click Ops): Great for learning services Inefficient in enterprise environment Reasons: Slow Error-prone Lacks consistency Main IaC Tools:\n1. AWS CloudFormation:\nBasic Concepts:\nUses templates (recipes, menus) to define AWS resources Supports YAML or JSON Stack:\nCollection of AWS resources deployed according to template Drift Detection:\nImportant feature that detects manual changes made on console (Click Ops) Changes not in template Ensures infrastructure always matches code 2. AWS CDK (Cloud Development Kit):\nFeatures:\nFramework using familiar programming languages: Python TypeScript Java C# Constructs:\nBasic building blocks Three levels: Level 1: Close to CloudFormation (L1 constructs) Level 2: High-level abstractions (L2 constructs) Level 3: Ready-made architectural patterns Process:\nCDK converts code to CloudFormation template Then deploys to AWS 3. Terraform:\nFeatures:\nMulti-cloud IaC tool Can be used for: AWS Azure GCP Language:\nUses HCL (HashiCorp Configuration Language) Considered easy to understand Terraform Plan:\nImportant feature Allows preview of changes that will occur Before applying to infrastructure Better control D. Container Services and Monitoring Container Services:\nDocker:\nBasic containerization platform Microservices:\nDistributed application architecture Amazon ECR (Elastic Container Registry):\nImage storage Orchestration Services:\nAmazon ECS (Elastic Container Service) Amazon EKS (Elastic Kubernetes Service) AWS App Runner:\nSimplified container deployment solution Quick containerized application deployment Monitoring \u0026amp; Observability:\nCloudWatch:\nMetrics collection Log management Alarm configuration Dashboard creation AWS X-Ray:\nProvides distributed tracing capability Understand service performance Analyze bottlenecks Importance of Monitoring:\nBuilding monitoring system is essential Evaluate system performance Example: Website runs under 1 second, under 5 seconds Measure and improve user experience 3. BEST PRACTICES A. T-shaped Skills T-shaped Model:\nVertical (Depth): Deep dive into specific area Example: Linux, Docker Horizontal (Breadth): Expand general knowledge Example: Kubernetes, AI application support Benefits:\nSpecialized in one field Capable of diverse work Good communication with other teams B. Focus on Fundamentals Important Starting Point:\nShould start from foundational knowledge: System Fundamental: Understand systems, networking, OS Developer Fundamental: Programming, algorithms, design patterns Why Important:\nStrong foundation helps learn new technologies faster Deeper understanding of how tools work C. Documentation Importance:\nRecord entire process, features, and structure Share knowledge within team Avoid having to repeatedly explain to other members Good Documentation Criteria:\nWrite clearly Even non-technical people can understand Include specific examples Update regularly D. Practice Learning Principles:\nAvoid: Watching tutorial videos and moving on Should: Practice hands-on Reasons:\nPractice reveals many emerging problems Helps understand technology more deeply With AI-generated Code:\nAvoid: Copying AI-generated code without understanding Should: Read and understand code, test thoroughly E. Project Management During development (e.g., project work):\nUse task management tools:\nJira Trello GitHub Projects Components to manage:\nTask breakdown: Break work into small tasks Backlog management: List of work to be done Future roadmap: Long-term development plan Benefits:\nClear progress tracking Efficient work allocation Easy review and improvement F. Soft Skills Importance:\nSoft skills are very important for technical people Especially in DevOps role Communication Skills:\nAbility to convey information clearly and understandably DevOps acts as bridge between groups: Development team Operations team Business stakeholders Necessary Soft Skills:\nCommunication Collaboration Problem-solving Empathy Presentation 4. KEY TAKEAWAYS 1. DevOps is Culture, Not Just Tools DevOps success requires: Right mindset Collaboration between teams Automation and continuous improvement 2. Infrastructure as Code is Mandatory Cannot rely on Click Ops in production environment IaC ensures: Consistency Reproducibility Version control 3. Automated Testing is Essential Even with AI-generated code, testing is needed CI/CD pipeline must include: Unit tests Integration tests Security scans Performance tests 4. Monitoring is Continuous Process Not \u0026ldquo;set and forget\u0026rdquo; Need to: Track metrics continuously Analyze trends Improve based on data 5. T-shaped Skills Help Career Development Specialize in one field Expand diverse knowledge Increase value in job market 5. APPLICATION TO WORK A. Build Complete CI/CD Pipeline Apply CodeCommit, CodeBuild, CodeDeploy, CodePipeline Implement automated testing Use Blue/Green or Canary deployment B. Transition to Infrastructure as Code Start with CloudFormation or Terraform Document entire infrastructure Version control all IaC code C. Containerize Applications Use Docker for containerization Deploy to ECS or EKS Implement proper logging and monitoring D. Improve Observability Setup CloudWatch dashboards Implement X-Ray tracing Define meaningful alarms E. Build Documentation Culture Document architecture decisions Write runbooks Share knowledge with team 6. CONCLUSION The \u0026ldquo;Mastering AWS DevOps: CI/CD, IaC, and Containers\u0026rdquo; workshop provided comprehensive knowledge about:\nDevOps mindset and principles CI/CD implementation on AWS Infrastructure as Code with CloudFormation, CDK, Terraform Container orchestration and monitoring Best practices for DevOps engineers Key Highlight:\nDevOps is not just about using tools or automation, but a work culture that requires collaboration between teams, thinking about automation and continuous improvement, along with good communication skills to bridge Development and Operations.\nThis workshop is an important foundation that helps:\nUnderstand modern DevOps practices Apply AWS services effectively Develop T-shaped skills Prepare for DevOps Engineer career Some event photos Add your event photos here\n"},{"uri":"https://thanhtai15.github.io/Inter_report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"During my internship at AWS, I participated in translating technical blog posts from English to Vietnamese to share knowledge about Amazon Q Developer with the Vietnamese user community. Below is the list of 3 blogs I translated:\nBlog 1 - European Region Announcement for Amazon Q Developer This blog announces the official launch of Amazon Q Developer Pro Tier in the Frankfurt region (eu-central-1), marking an important milestone for European customers. I learned about two main benefits: EU data residency which helps meet compliance requirements, and performance optimization with reduced latency by processing requests closer to users. The article also explains multi-region inference and how Amazon Q uses multiple European regions (Frankfurt, Ireland, Paris, Stockholm) to optimize processing.\nKey Learnings:\nThe importance of data residency in complying with EU regulations Multi-region architecture and latency optimization Generally Available (GA) features of Amazon Q Developer by region How to handle multi-region requests in AWS global environment Blog 2 - Combining Snyk Insights with Amazon Q Developer Support This blog introduces the integration between Snyk (a leading security platform for developers) and Amazon Q Developer to streamline secure software development workflows. I learned how these two tools work in tandem: Snyk detects security issues in code, dependencies, container images, and cloud infrastructure, while Amazon Q Developer assists in quick remediation with AI.\nThe article provides detailed guidance on:\nInstalling and configuring both IDE plugins (Visual Studio Code, JetBrains, Eclipse, Visual Studio) Snyk\u0026rsquo;s automatic codebase scanning and vulnerability detection Using Amazon Q Developer to receive security fix suggestions Integrated workflow between security scanning and AI-assisted remediation Key Learnings:\nSecurity-first development approach in DevSecOps Integrating security tools into IDE workflow Best practices for vulnerability management The role of AI in accelerating security remediation Blog 3 - Introducing Quick Start Experience for Amazon Q Developer Pro This blog introduces a new, simplified setup experience for Amazon Q Developer Pro Tier. I learned about the requirements and benefits of Pro Tier: higher limits, enterprise-grade governance, analytics dashboard, code customization, and IP indemnification.\nThe article addresses common scenarios when:\nOrganization hasn\u0026rsquo;t deployed IAM Identity Center organization-wide Want to deploy for a separate user group Don\u0026rsquo;t control the AWS Organization (managed by third party) The two-step quick start experience helps teams easily try Amazon Q Developer Pro in their IDE without complex IAM Identity Center setup at the organization level.\nKey Learnings:\nDifferences between Amazon Q Developer Free Tier and Pro Tier AWS IAM Identity Center: organization instance vs. standalone instance Enterprise governance and user management in AWS Flexible deployment strategies for AI developer tools Skills Gained from Blog Translation:\nTechnical Writing: Enhanced ability to articulate complex technical terms (data residency, multi-region inference, vulnerability management) clearly and understandably Amazon Q Developer Knowledge: Deep understanding of AI-powered development assistant, Pro Tier features, and IDE integration Cloud Security Concepts: Mastery of DevSecOps, security scanning, vulnerability remediation, and compliance requirements AWS Services: Learning about IAM Identity Center, AWS Organizations, regional architecture, and enterprise governance Vietnamese Localization: Learning to \u0026ldquo;Vietnamize\u0026rdquo; technical content while maintaining original meaning and professionalism Research Skills: Exploring documentation, best practices, and real-world use cases to translate context accurately Attention to Detail: Ensuring accuracy of technical terminology, workflow descriptions, and configuration steps(3.6-Blog6/) This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA. "},{"uri":"https://thanhtai15.github.io/Inter_report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Deploy and manage EC2 Instances in complex VPC environment: Create EC2 Instances in different Subnets Configure Transit Gateway to connect multiple VPCs Set up VPC Endpoints for private connections Master advanced EC2 concepts: Amazon Machine Images (AMI) and Backup management Distinguish between EBS (Elastic Block Store) and Instance Store EC2 Auto Scaling for automatic system expansion User Data and Meta Data in EC2 Get familiar with AWS Storage and Content Delivery: Create and manage S3 Bucket Host static website on S3 Use CloudFront CDN to speed up access Begin Machine Learning and NLP journey: Learn about Supervised Machine Learning Sentiment Analysis and Natural Language Processing Build simple Logistic Regression models Tasks to implement this week: Day Task Start Date End Date Resources Mon - Create EC2 Instances in Subnets - Practice creating Internet Gateway - Learn about Transit Gateway Route Tables - Install Transit Gateway - Connect EC2 Instance to Endpoint 22/09/2025 22/09/2025 https://www.youtube.com/@AWSStudyGroup/ https://cloudjourney.awsstudygroup.com/ Tue - Deep dive into EC2 + AMI/ Backup/ Key pair + Elastic Block Store (EBS) + Instance Store + User data \u0026amp; meta data + EC2 Auto Scaling 23/09/2025 23/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/@AWSStudyGroup/ Wed - Deploy Infrastructure - Create Backup plan - Conduct Recovery Testing - Clean up resources - Create S3 Bucket 24/09/2025 24/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/@AWSStudyGroup/ Thu - Create EC2 for Storage Gateway - Practice creating a simple static website - Configure public access block and public objects - Learn about AWS CloudFront and practice configuring CloudFront 25/09/2025 25/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/@AWSStudyGroup/ Fri - Learn Supervised ML \u0026amp; Sentiment Analysis - Natural Language preprocessing - Visualizing tweets and Logistic Regression models - Team meeting to brainstorm ideas for the final project 26/09/2025 26/09/2025 https://www.coursera.org/learn/classification-vector-spaces-in-nlp/ Week 3 Achievements: Successfully deployed complex EC2 infrastructure:\nCreated and configured EC2 Instances in different Subnets Set up Transit Gateway and Route Tables Connected EC2 to VPC Endpoints Mastered advanced EC2 concepts:\nDistinguished between EBS (Elastic Block Store) and Instance Store Understood AMI, Backup and Key pair management Learned about User data, Meta data and EC2 Auto Scaling Practiced with AWS Backup:\nCreated Backup plan for EC2 Performed restore and tested data recovery Deployed storage and content delivery solutions:\nCreated and configured S3 Bucket Hosted static website on S3 Configured CloudFront CDN to speed up access Started Machine Learning research:\nLearned about Supervised ML and Sentiment Analysis Practiced Natural Language Preprocessing Built Logistic Regression models Defined final project idea with the team\n"},{"uri":"https://thanhtai15.github.io/Inter_report/5-workshop/5.4-databaseandstorage/","title":"Database And Storage","tags":[],"description":"","content":"Overview This project used three data storage services which are: AWS RDS(Relational Database Service) is a AWS service to config database and is located in Private Subnet AWS S3 (Simple Storage Service) is a AWS service to storage picture and video AWS ECR (Elastic Container Registry) is a AWS service to storage docker image Content Create RDS Create S3 Create ECR "},{"uri":"https://thanhtai15.github.io/Inter_report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Master Hybrid Cloud workflows - connecting On-premises and AWS Cloud: Export and import Virtual Machines (VMs) between VMWare and AWS Create AMIs from imported VMs and deploy EC2 Instances Learn about VM Import/Export Service and technical requirements Deepen knowledge of AWS Storage Services: EFS (Elastic File System) - shared file storage FSx - fully managed file systems (Windows File Server, Lustre) Storage Gateway - connecting on-premises storage with AWS EC2 Autoscaling to automatically adjust number of instances Lightsail - simple VPS solution for beginners Build mathematical foundation for NLP and Machine Learning: Vector Spaces and mathematical operations Euclidean Distance \u0026amp; Cosine Similarity for measuring similarity Probability and Bayes\u0026rsquo; Rule in text classification Linear Algebra with Python Numpy Finalize semester project concept and setup documentation framework: Choose suitable Hugo theme Learn how to write Workshop report using Hugo Agree on final idea with team Tasks to implement this week: Day Task Start Date End Date Resources Mon - Learn about EC2 Autoscaling - EFS/FSx - Lightsail - Supplement knowledge on Basic Storage \u0026amp; Compute services on AWS - Learn about Probability and Bayes’ Rule 29/09/2025 29/09/2025 https://www.youtube.com/@AWSStudyGroup/ https://www.coursera.org/learn/classification-vector-spaces-in-nlp/ Tue - Expand knowledge on Database and Security services on AWS - Learn VMWare Workstation - Practice exporting Virtual Machine from On-premises - Practice uploading Virtual Machine to AWS 30/09/2025 30/09/2025 https://www.youtube.com/@AWSStudyGroup/ Wed - Learn Linear algebra in Python with Numpy - Learn Euclidean Distance \u0026amp; Cosine Similarity - Learn Manipulating Words in Vector Spaces - Practice code labs on Vector Space Models 01/10/2025 01/10/2025 https://www.coursera.org/learn/classification-vector-spaces-in-nlp/ Thu - Import Virtual Machine into AWS - Deploy Instance from AMI - Configure S3 bucket ACLs - Export VM from Instance - Clean up AWS Cloud resources - Create Storage Gateway - Mount File shares on On-premises machine 02/10/2025 02/10/2025 https://www.youtube.com/@AWSStudyGroup/ Fri - Learn about Hugo Themes - Learn how to write the Final Workshop Report - Summary meeting and finalize the ultimate idea for the final report with the group 03/10/2025 03/10/2025 https://www.youtube.com/@AWSStudyGroup/ Week 4 Achievements: Successfully implemented Hybrid Cloud strategies:\nExported VMs from local VMWare Workstation and uploaded them to AWS. Imported VMs to create AMIs and launched functional Instances. Exported AWS Instances back to local environments. Configured Advanced Storage Solutions:\nDeployed AWS Storage Gateway. Successfully mounted AWS file shares directly onto on-premises machines. Managed S3 Access Control Lists (ACLs) for granular security. Applied Mathematical Foundations for NLP (Machine Learning):\nUtilized Python (Numpy) for Linear Algebra operations. Calculated Euclidean Distance \u0026amp; Cosine Similarity for text analysis. Understood and applied Bayes’ Rule and Vector Space Models. Project \u0026amp; Documentation Readiness:\nSelected and configured the Hugo Theme for the final report. Reached a consensus on the final project idea with the team. "},{"uri":"https://thanhtai15.github.io/Inter_report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 - AWS AI/ML and Generative AI Workshop Date \u0026amp; Time: Saturday, November 15, 2025, from 8:30 AM to 12:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nMain Content:\nSession 1 (9:00-10:30 AM): AWS AI/ML Services Overview with Amazon SageMaker\nData preparation and labeling Model training and fine-tuning Model deployment and MLOps Live Demo: SageMaker Studio Session 2 (10:45 AM-12:00 PM): Generative AI with Amazon Bedrock\nComparing Foundation Models (Claude, Llama, Titan) Prompt Engineering techniques (Chain-of-Thought, Few-shot learning) Retrieval-Augmented Generation (RAG) architecture Bedrock Agents and Guardrails Knowledge Gained:\nUnderstanding AI-Driven Development Lifecycle (AIDLC) and Human-Centric philosophy Mastering end-to-end ML development with SageMaker Learning to build GenAI applications with RAG pattern Prompt engineering techniques and model selection Value Obtained:\nReduced development time from 2 weeks to 1.5 days with AIDLC Understanding MLOps and production deployment best practices Knowledge of Responsible AI and content filtering with Guardrails Event 2 - GenAI-powered App-DB Modernization \u0026amp; DevOps on AWS Workshop Date \u0026amp; Time: Saturday, November 16, 2025\nLocation: AWS Vietnam Office\nRole: Attendee\nSpeakers:\nLam Thu Kiet - Senior DevOps Engineer at FPT Software (banking domain, chatbot deployment) Danh Hoang Hieu Nghi - AI Engineer at Renob (AI deployment best practices) Main Content:\nDevOps Mindset: Culture, principles, and DORA metrics (Deployment Frequency, Lead Time, MTTR) CI/CD Pipeline: AWS CodeCommit, CodeBuild, CodeDeploy, CodePipeline Deployment Strategies: Blue/Green, Canary, Rolling updates Infrastructure as Code: CloudFormation with Drift Detection AWS CDK with 3-level Constructs (L1, L2, L3) Terraform with Plan feature and HCL language Container Services: ECS, EKS, App Runner comparison Monitoring \u0026amp; Observability: CloudWatch, X-Ray, distributed tracing Best Practices: Automation, collaboration, continuous improvement Knowledge Gained:\nDevOps is not just tools but culture connecting Dev and Ops Comparing 3 IaC tools and knowing when to use which Understanding Click Ops problem and why IaC is needed Container orchestration strategies and service selection Value Obtained:\nMastering CI/CD best practices on AWS Understanding drift detection and state management in IaC Real-world experience from senior engineers about production challenges Importance of monitoring in DevOps culture Event 3 - Mastering AWS DevOps: CI/CD, IaC, and Containers Date \u0026amp; Time: Monday, November 17, 2025, from 8:30 AM to 5:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nMain Content:\nDevOps Principles:\nDevOps bridges Development and Operations Automation to reduce human errors DORA metrics, MTTR, Deployment Frequency CI/CD Deep Dive:\nCI vs CD vs Continuous Deployment distinctions AWS CodePipeline architecture Automated testing and quality gates Infrastructure as Code:\nCloudFormation: declarative, AWS-native, free CDK: programmatic approach, reusability Terraform: multi-cloud, large community, HCL language Best practices: version control, modular design, documentation Container Technologies:\nDocker fundamentals and containerization benefits ECS: AWS-managed, tight integration EKS: Kubernetes compatibility, complex but powerful App Runner: Simplest option for containerized apps Best Practices:\nT-shaped skills: Deep expertise + broad knowledge Documentation culture: Knowledge sharing Practice over theory: Hands-on learning Soft skills: Communication, teamwork, problem-solving Knowledge Gained:\nDevOps is culture, not just tools Importance of fundamental knowledge before learning tools Systematic approach to problem-solving Project management with Jira/Trello Value Obtained:\nDeep understanding of 3 IaC tools and trade-offs of each Container orchestration strategies for different use cases T-shaped skills mindset for career development Soft skills importance in technical roles Summary of 3 Events:\nThrough these 3 workshops, I gained comprehensive insights into:\nAI/ML Development: From model training to production deployment with SageMaker and Bedrock DevOps Culture: Automation, collaboration, and continuous improvement mindset Modern Cloud Architecture: CI/CD, IaC, containers, and observability Best Practices: From senior engineers with real-world experience This knowledge not only complements my daily work but also provides clear direction for my career path in Cloud and DevOps.\n"},{"uri":"https://thanhtai15.github.io/Inter_report/5-workshop/5.5-computeandcontainer/","title":"Compute And Container","tags":[],"description":"","content":"IAM Role Create ecsTaskExecutionRole role to AWS ECS (Elastic Container Service) pull docker image and write logs from ECR repository Open the Amazon IAM In left navbar, choose Roles, then click Create role In create console, choose AWS service and select Use case is EC2 then click Next In Add permissions step, search AmazonECSTaskExecutionRolePolicy and select this Policy, then click Next In Name, review, and create, fill in Role name is ecsTaskExecutionRole , then click Create role Create FargateTaskRole role to allow Fargate access resource In left navbar, choose Roles, then click Create role In create console, choose AWS service and select Use case is EC2 then click Next In Add permissions step, search AmazonS3FullAccess, AmazonSESFullAccess and AmazonSNSFullAccess, then click Next Review and fill in Role name is FargateTaskRole, then click Create role ECS Cluster Create ECS Cluster is place to Fargate running Open the Amazon ECS In left navbar, choose Clusters, then click Create Cluster In create console, fill in Cluster name In Infrastructure, select Fargate only Then click Create Task definitions Create Task definitions is a container design In left navbar of ECS console, choose Task definitions, then click **Create new task definitions In create console, fill in Task definition family Select AWS Fargate in Launch type of Infrastructure requirements Choose Task role is FargateTaskRole Choose Task execution role is ecsTaskExecutionRole In Container, fill in name and URL of ECR, then Add Environment variable Scroll down to the bottom and click Create ECS Service Create ECS Service to run Task definitions In left navbar, choose Clusters, then click Cluster created In tab Sevices, click Create In create console, choose Task definition family Choose Task definition revision Fill in Service name In Compute options choose Capacity provider strategy In Capacity provider, select FARGATE In Platform version, select LATEST In Deployment configuration, Scheduling strategy is Replica and Desired tasks is 1 Then click Create "},{"uri":"https://thanhtai15.github.io/Inter_report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Master advanced NLP algorithms and Machine Learning: Vector Transformation for word translation K-nearest neighbors (KNN) for text classification Hash tables and hash functions for fast search Practice Word Translation lab with word embeddings Deep dive into AWS Storage ecosystem - comprehensive data storage: Advanced S3 features: Access Points, Storage Classes, Versioning S3 Static Website hosting and CORS configuration Object Key naming and Performance optimization Amazon Glacier for long-term cost-effective storage AWS Snow Family (Snowcone, Snowball, Snowmobile) for data migration AWS Backup for centralized backup strategy Implement Security compliance and Automation: AWS Security Hub for compliance assessment IAM Roles and Policies for EC2 and Lambda AWS Lambda functions for task automation Resource tagging strategy for management and cost allocation Finalize detailed execution plan for final semester project: Define system architecture Divide work and timeline Choose technologies and AWS services to use Tasks to implement this week: Day Task Start Date End Date Resources Mon - Learn about Transforming word vectors - Learn K-nearest neighbors - Learn about Hash tables and hash functions - Complete Word Translation code lab 06/10/2025 06/10/2025 https://www.coursera.org/learn/classification-vector-spaces-in-nlp/ Tue - Learn AWS Storage Services - Learn Amazon Simple Storage Service (S3) - Access Points - Storage Classes - Learn S3 Static Website \u0026amp; CORS - Control Access - Object Key \u0026amp; Performance - Glacier - Learn Snow Family - Storage Gateway - Backup 07/10/2025 07/10/2025 https://www.youtube.com/@AWSStudyGroup/ Wed - Complete Lab Module 05 + Enable Security Hub + Score for each set of criteria + Clean up resources + Create VPC + Create Security Group and EC2 Instance + Tag Instance + Create Role for Lambda + Create Lambda Function + Check results and clean up 08/10/2025 08/10/2025 https://www.youtube.com/@AWSStudyGroup/ Thu - Translate Blog - Meeting to build the final project plan - Complete Lab Module 05-28 - Complete Lab Module 05-29 - Complete Lab Module 05-30 09/10/2025 09/10/2025 https://www.youtube.com/@AWSStudyGroup/ Fri - Complete Lab Module 05-31 - Complete Lab Module 05-32 - Complete Lab Module 05-33 - Team meeting to summarize the final project construction plan - Review knowledge from the week 10/10/2025 10/10/2025 https://www.youtube.com/@AWSStudyGroup/ Week 5 Achievements: [Image of AWS Security Hub architecture]\nMastered Vector Space arithmetic in NLP:\nImplemented K-nearest neighbors (KNN) and Hash tables. Successfully coded a Word Translation model using vector transformation. Configured Advanced AWS Storage Architectures:\nDeployed S3 Static Websites with CORS configurations. Managed S3 Storage Classes and Access Control. Understood offline data migration using AWS Snow Family. Implemented Cloud Security and Automation:\nEnabled and scored compliance using AWS Security Hub. Created Serverless automation workflows using AWS Lambda and IAM Roles. Managed EC2 resources via tagging strategies. Project Progress:\nCompleted extensive lab series (Module 05). Finalized and agreed upon the detailed roadmap for the final project. "},{"uri":"https://thanhtai15.github.io/Inter_report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Deploy ReGenZet Management System To AWS Overview ReGenZet is an enterprise-grade EV garage management platform. The objective of this workshop is to design and deploy a secure, cost-optimized, and highly automated cloud infrastructure on AWS to host ApexEV\u0026rsquo;s frontend, backend, storage, and serverless AI/ML functions.\nKey architectural principles:\nSecurity-first: least-privilege IAM, encrypted data at rest and in transit, network isolation and controlled service endpoints. Cost optimization: use managed services with pay-as-you-go models, right-sizing, and automated lifecycle policies for storage and compute. Automation \u0026amp; Observability: Infrastructure-as-Code, CI/CD pipelines, centralized logging, and automated monitoring/alarms. Core services used in this workshop:\nAWS ECS (Fargate) — run backend microservices without managing servers. AWS Amplify — host the frontend, provide CI/CD for web clients and manage hosting. Amazon RDS — managed relational database for transactional data. Amazon S3 — object storage for media, backups, and static assets. AWS Lambda — serverless functions for AI/ML processing pipelines, notifications and background tasks. This workshop contains hands-on modules covering the end-to-end stack and best practices for each layer.\nContent Workshop Overview Project Architecture VPC Of Project Database And Storage Compute And Container Create Load Balancing Create Amplify And API Gateway Intruct Deploy Code Frontend and Backend "},{"uri":"https://thanhtai15.github.io/Inter_report/5-workshop/5.7-amplifyandapigateway/","title":"Create Amplify And API Gateway","tags":[],"description":"","content":"AWS Amplify Create Amplify to deploy Frontend Open the Amazon Amplify Click Create new app In create console, choose Gitlab, then click Next In step Add repository and branch, login with Gitlab, then choose Git Repository and branch need to deploy Then click Next Setting format with your type code in your Frontend, then click Next Review and click Save and deploy After this step wait about 3-5 minutes to deploy and you can access your app from internet API Gateway API Gateway is service to transfer HTTP and HTTPS between Amplify (Frontend) and Fargate (Backend) Open the Amazon API Gateway Click Create API Choose REST API and click Build In create console, choose API details is New API Fill in API name API endpoint type is Regional Security policy is SecurityPolicy_TLS13_1_2_2021_06 Then click Create API In left navbar, choose APIs and select API Gateway created In console API Gateway click Create Resource then fill in Resource name and click Create resource In console resource API Gateway, click Create method In Method details choose ANY, and Integration type is HTTP Proxy HTTP method select ANY, and Endpoint URL is Endpoints of ALB Then scroll down to the bottom and click Create method In resource proxy, create method In create console, choose Integration type is Mock and Method type is OPTIONS Then click Create method Now we finish setup API Gateway for project "},{"uri":"https://thanhtai15.github.io/Inter_report/5-workshop/5.6-createalb/","title":"Create AWS Load Balancers","tags":[],"description":"","content":" Open the Amazon EC2 In left navbar, choose Load Balancers, then click Create load balancer Choose Application Load Balancer In create console, fill in Load balancer name Scheme is Internet-facing, then select VPC In Availability Zones and subnets choose two AZ and select two subnet in public\\ Security groups select alb-sg Then scroll down to the bottom and click Create load balancer "},{"uri":"https://thanhtai15.github.io/Inter_report/5-workshop/5.8-instruct-deploy-be-fe/","title":"Instruct Deploy BackEnd And Frontend","tags":[],"description":"","content":"Deploy Frontend Instructure Deploy Frontend to Amplify Open terminal in your code from your compute Commit and Push to branch of Gitlab which define in Amplify Amplify auto CICD and deploy your Frontend Wait 3-5 minutes Deploy Backend Instructure Deploy Backend to Fargate Open the Amazon ECR In left navbar, choose Repository and select ECR repository created Open terminal in your code from your compute Write syntax \u0026ldquo;AWS configure\u0026rdquo; to setup access key and secret key of your AWS account After setup AWS account for CLI, comeback console of ECR Repository Click View push command, you will see syntax to push docker image to ECR repository Then copy and paste it to terminal of your Backend project Fisnish this step, AWS ECS Fargate will auto pull and run image have tag latest Finally,Congratulations for finish this workshop with deploy success project to AWS. "},{"uri":"https://thanhtai15.github.io/Inter_report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Master AWS Security fundamentals: Shared Responsibility Model - dividing responsibilities between AWS and customers AWS Identity Center (AWS SSO) for centralized access management AWS Organizations to manage multiple AWS accounts Amazon Cognito for user authentication in applications AWS KMS (Key Management Service) for data encryption AWS Security Hub for centralized security monitoring Develop skills in designing and visualizing Cloud Architecture: Use Draw.io with official AWS Icons Understand cloud-native architecture design principles Draw High-Level Architecture and Detailed Architecture diagrams Apply best practices in distributed system design Deep dive into NLP Attention Models - attention mechanism in Neural Networks: Seq2seq (Sequence-to-Sequence) model and how it works Queries, Keys, Values in Attention Mechanism Neural Machine Translation (NMT) for automatic translation BLEU Score and ROUGE-N Score to evaluate translation quality Beam Search and Minimum Bayes Risk sampling strategies Collaborate with team to draft and refine High-Level Architecture: Identify AWS services to be used Design data flow and integration points Receive feedback from mentor and adjust diagram Tasks to implement this week: Day Task Start Date End Date Resources Mon - Learn how to draw AWS architecture diagrams - Get familiar and practice drafting with Draw.io - Complete Lab Module 05-44 - Complete Lab Module 05-48 13/10/2025 13/10/2025 https://www.youtube.com/@AWSStudyGroup/ Tue - Learn Shared Responsibility Model - Learn Amazon Identity \u0026amp; Access Management (IAM) - Learn AWS Cognito - Learn AWS Organizations - Learn AWS Identity Center - Learn Amazon Key Management Service (KMS) - Learn AWS Security Hub - Research, practice, and supplement daily learning 14/10/2025 14/10/2025 https://www.youtube.com/@AWSStudyGroup/ Wed - Meeting to draft the overview architecture diagram of services to be used - Learn Seq2seq model - Learn Queries, Keys, Values, and Attention - Learn Seq2seq Model with Attention - Learn NMT Model, Machine Translation - Learn BLEU Score \u0026amp; ROUGE-N Score - Learn Beam Search \u0026amp; Minimum Bayes Risk 15/10/2025 15/10/2025 https://www.coursera.org/learn/attention-models-in-nlp/ Thu - Learn the structure of AWS architecture diagrams - Practice drawing and editing diagrams 16/10/2025 16/10/2025 https://www.youtube.com/@AWSStudyGroup/ Fri - Review knowledge from the week - Continue editing and asking for feedback from mentors/seniors regarding the group architecture diagram - Group meeting to discuss the AWS architecture diagram 17/10/2025 17/10/2025 https://www.youtube.com/@AWSStudyGroup/ Week 6 Achievements: Solidified Cloud Security Knowledge:\nUnderstood the Shared Responsibility Model. Configured IAM and AWS Identity Center for user management. Explored AWS Cognito for app authentication and KMS for encryption. Professional Architecture Design:\nProficient in using Draw.io with AWS Icons sets. Drafted the initial High-Level Architecture for the final project. Received and implemented feedback on the diagram structure. Mastered Advanced NLP Concepts (Attention Models):\nUnderstood the mechanics of Seq2seq and the Attention mechanism (Queries, Keys, Values). Learned to evaluate Machine Translation models using BLEU and ROUGE-N scores. Grasped sampling techniques like Beam Search. Team Collaboration:\nUnified the team\u0026rsquo;s vision on the system architecture through structured meetings. "},{"uri":"https://thanhtai15.github.io/Inter_report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"Overview of Internship Experience Throughout my internship at Amazon Web Services (AWS) from September 8, 2024 to November 14, 2024, I had a valuable opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment at one of the world\u0026rsquo;s leading technology companies.\nProject Participation I participated in the Electric Vehicle Maintenance Management System project, which provided me with practical experience in:\nTeamwork skills: Effective team collaboration, coordinating with team members to complete sprints and milestones Programming skills: Applying AWS cloud technologies, developing fullstack applications with best practices Reporting skills: Creating technical documentation, writing weekly reports, and presenting work progress Work Attitude and Professionalism Throughout the internship, I consistently maintained a professional working attitude:\nSense of responsibility: Successfully completing assigned tasks by deadlines Compliance with regulations: Strictly adhering to company policies regarding schedules and information security Collaborative spirit: Actively communicating, supporting colleagues, and learning from mentors Proactive learning: Continuously exploring and researching new technologies to improve skills Detailed Self-Assessment To objectively and comprehensively reflect on my internship experience, I evaluate myself based on 12 important criteria below. Each criterion is assessed at three levels: Good, Fair, and Average.\nNo. Criteria Detailed Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Absorbing new knowledge, learning quickly, adapting to new technologies ☐ ✅ ☐ 3 Proactiveness Taking initiative, accepting tasks without waiting for instructions, showing initiative at work ✅ ☐ ☐ 4 Sense of responsibility Completing work on time, ensuring quality, committing to deadlines ✅ ☐ ☐ 5 Work discipline Adhering to schedules, company regulations, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback, self-improvement, continuous learning ☐ ✅ ☐ 7 Communication skills Presenting ideas, reporting work clearly, effective expression ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues, supporting team members, team spirit ✅ ☐ ☐ 9 Professional conduct Respecting colleagues and partners, maintaining a positive work environment ✅ ☐ ☐ 10 Problem-solving thinking Identifying issues, analyzing root causes, proposing creative solutions ☐ ✅ ☐ 11 Contribution to project/team Actual work effectiveness, improvement initiatives, team evaluation ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period, overall performance ✅ ☐ ☐ Strengths Analysis Based on the assessment table above, I recognize my strengths include:\n1. Strong Professional Skills (Good)\nSuccessfully applied knowledge of AWS Cloud Services to real projects Mastered technologies: Docker, CI/CD, Infrastructure as Code Completed technical tasks with good quality 2. High Proactiveness (Good)\nSelf-studied AWS documentation when encountering issues Proposed project improvements without requiring direction Proactive in learning new technologies 3. Effective Teamwork (Good)\nCoordinated well with team members in sprint planning Willing to support colleagues when they faced difficulties Actively contributed to daily standups and retrospective meetings 4. Work Responsibility (Good)\nCompleted 100% of assigned tasks on deadline Ensured code quality through code review process Committed to sprint goals and deliverables Areas for Improvement Alongside these strengths, I recognize some limitations that need to be addressed:\n1. Work Discipline (Average)\nIssue: Sometimes did not strictly adhere to work schedules Root cause: Poor personal time management habits Improvement plan: Develop early sleeping and waking habits Use time management tools (calendar, reminders) Set goal to arrive at office 15 minutes before work hours 2. Problem-Solving Thinking (Fair)\nIssue: Lack systematic thinking in debugging and troubleshooting Root cause: Limited practical experience, no framework approach Improvement plan: Learn methodologies: Root Cause Analysis, 5 Whys, Fishbone Diagram Practice with complex coding challenges Observe how senior engineers solve problems 3. Communication Skills (Fair)\nIssue: Sometimes ideas are not presented clearly and coherently Root cause: Insufficient preparation before presentations, lack of confidence speaking before groups Improvement plan: Attend presentation skills training sessions Practice writing structured documents (using STAR method) Volunteer to present in team meetings Learn to use visual aids (diagrams, slides) more effectively Development Commitment Based on this objective self-assessment, I commit to:\nShort-term (1-3 months):\nImprove work time discipline to \u0026ldquo;Fair\u0026rdquo; level Attend at least 2 soft skills training sessions Complete 1 online course on problem-solving frameworks Medium-term (3-6 months):\nElevate problem-solving thinking to \u0026ldquo;Good\u0026rdquo; level Improve communication skills through regular practice Contribute more to technical discussions Long-term (6-12 months):\nBecome a role model in discipline for junior members Develop ability to lead technical presentations Progress from junior to mid-level engineer I believe that with effort and determination, these areas for improvement will be addressed in the near future.\n"},{"uri":"https://thanhtai15.github.io/Inter_report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Master AWS Database services: Amazon RDS (Relational Database Service) - managed relational database Amazon Aurora - MySQL and PostgreSQL compatible with high performance Amazon Redshift - data warehouse for large-scale analytics Amazon ElastiCache (Redis, Memcached) - in-memory caching Distinguish Online Transaction Processing (OLTP) vs Online Analytical Processing (OLAP) Develop practical AI skills - real-world application: Research chatbot types and use cases Design conversation flow and intent detection Learn about AI platforms: Amazon Lex, Amazon Comprehend Build AI Chatbot prototype with Python Integrate Chatbot into project architecture Finalize High-Level Architecture Diagram: Adjust diagram based on mentor feedback Add database and AI services to architecture Finalize system design version Kick-start Final Project implementation - Implementation phase: Design Frontend (UI/UX) with React or Vue.js Design Backend API architecture with RESTful or GraphQL Setup development environment and Git workflow Start coding basic modules Tasks to implement this week: Day Task Start Date End Date Resources Mon - Complete Lab Module 06 - Continue editing the Architecture Diagram 20/10/2025 20/10/2025 https://www.youtube.com/@AWSStudyGroup/ Tue - Learn Database Concepts - Learn about Amazon RDS \u0026amp; Amazon Aurora - Learn about Redshift - Elasticache - Learn how to create an AI chatbot 21/10/2025 21/10/2025 https://www.youtube.com/@AWSStudyGroup/ Wed - Practice creating an AI chatbot - Research and supplement AI knowledge regarding chatbots - Revise the diagram based on reviews from mentors in the group 22/10/2025 22/10/2025 https://www.youtube.com/ Thu - Commence the Final Project - Design Front-End \u0026amp; Back-end 23/10/2025 23/10/2025 Fri - Review knowledge from the week - Continue editing and asking for feedback from group members regarding the architecture diagram - Test AI and design Front-end and Back-end parts 24/10/2025 24/10/2025 Week 7 Achievements: [Image of AWS RDS architecture]\nDeep understanding of AWS Database Ecosystem:\nDistinguished between Relational (RDS, Aurora) and Data Warehousing (Redshift) services. Understood In-memory caching strategies using Amazon ElastiCache. AI \u0026amp; Machine Learning Implementation:\nSuccessfully researched and prototyped a functional AI Chatbot. Integrated AI logic into the project scope. Architecture \u0026amp; Design Finalization:\nRefined and finalized the System Architecture Diagram after multiple rounds of feedback/review. Project Execution:\nOfficially launched the coding phase for the Final Project. Completed initial designs for both Frontend (UI/UX) and Backend structure. "},{"uri":"https://thanhtai15.github.io/Inter_report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment at AWS in the First Cloud Journey program is extremely professional and dynamic. The office is designed to international standards with full amenities, and the open workspace facilitates easy communication between team members. Notably, Amazon\u0026rsquo;s \u0026ldquo;Day 1\u0026rdquo; culture is truly applied in daily work - always maintaining an entrepreneurial spirit and continuous innovation.\nThe standout feature is the support from team members: everyone is always ready to answer questions, share experiences, and create opportunities for interns to learn. The working atmosphere is comfortable yet maintains professionalism, helping me balance work pressure and personal development.\n2. Support from Mentor / Team Admin\nThe mentors have a very effective coaching style following the \u0026ldquo;learn by doing\u0026rdquo; method. Instead of providing direct answers, mentors guide me on how to think, analyze problems, and find solutions myself. This helps me develop independent thinking and problem-solving skills sustainably. They listen and answer all questions in detail. Share best practices and real-world experiences from large projects. Provide constructive feedback to help me improve daily.\nThe admin team is also very attentive in supporting administrative procedures, answering questions about company policies, and creating the best conditions for the internship process.\n3. Relevance of Work to Academic Major\nThe assigned work has a high degree of relevance (90%) to the knowledge learned at school.\nIn particular, I was exposed to real AWS technologies and services such as EC2, S3, Lambda, RDS, CloudFormation - knowledge that was only theoretical at school. This helped me understand more deeply the practical application of what I learned, while expanding my vision of modern technology trends.\nHowever, there is still about 10% of work that requires me to self-study and research more (like Terraform, Kubernetes), but this is actually a good opportunity to practice self-learning skills - an essential skill in the IT industry.\n4. Learning \u0026amp; Skill Development Opportunities\nThe First Cloud Journey program provides comprehensive learning opportunities:\nTechnical Skills:\nProficiency in AWS Cloud Services: EC2, S3, Lambda, RDS, CloudFormation, CDK Understanding of CI/CD pipeline with AWS CodePipeline, CodeBuild, CodeDeploy Applying Infrastructure as Code (IaC) with CloudFormation and Terraform Container orchestration with Docker, ECS Monitoring and logging with CloudWatch Soft Skills:\nWorking in Agile/Scrum model with sprint planning, daily standup, retrospective Professional presentation and work reporting skills Effective communication in multicultural teams (English and Vietnamese) Time management and work prioritization 5. Company Culture \u0026amp; Team Spirit\nThe working culture at AWS is built on 16 Leadership Principles, of which I experienced most clearly:\nCustomer Obsession: Always putting customers first in every decision Ownership: Each person takes responsibility for their work as an owner Learn and Be Curious: Continuously learning and exploring new technologies Earn Trust: Building trust through transparency and responsibility The team spirit is excellent: people are willing to support each other regardless of position or time. When I encounter blocking issues, there is always someone ready for pair programming or participating in brainstorming to find solutions. The team regularly organizes:\nWeekly knowledge sharing sessions Code review sessions to learn from each other Lunch and learn sessions about new technologies I feel like I am truly part of the team, not just an intern but a valuable contributor.\n6. Internship Policies / Benefits\nThe company provides flexible working hours when needed. In addition, having the opportunity to participate in internal training sessions is a big plus.\nWords of Thanks: I would like to sincerely thank the FCJ team, mentors, and everyone who supported me throughout the internship. This is an extremely valuable experience that I will never forget. I hope the feedback above will help the program become better for future interns.\nI am very proud to be part of the First Cloud Journey program and hope to continue contributing to AWS in the future! 🚀\n"},{"uri":"https://thanhtai15.github.io/Inter_report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Finalize AI Chatbot and integrate into final project: Optimize chatbot response time and accuracy Experiment with different NLP models Integrate chatbot into backend API Test end-to-end conversation flow Write comprehensive and professional Project Proposal: Describe problem statement and solution approach List AWS services used and reasons for selection Present system architecture and data flow Define timeline and deliverables Complete basic Frontend and Backend: Build React components for UI Implement RESTful APIs for backend Connect Frontend-Backend and test integration Setup authentication and authorization basics Review for midterm exam: Consolidate all AWS knowledge learned (8 weeks) Review services: EC2, VPC, S3, RDS, Lambda Practice sample exercises and quizzes from AWS Study Group Master best practices and Well-Architected Framework Tasks to implement this week: Day Task Start Date End Date Resources Mon - Revise architecture diagram and change some services - Finalize AI and integrate into project 27/10/2025 27/10/2025 Tue - Write proposal - Team meeting to work on project - Complete basic Front-end and Back-end 28/10/2025 28/10/2025 Wed - Review all learned knowledge - Review additional supplementary knowledge 29/10/2025 29/10/2025 https://www.youtube.com/@AWSStudyGroup/ https://cloudjourney.awsstudygroup.com/ Thu - Review all learned knowledge - Review additional supplementary knowledge 30/10/2025 30/10/2025 https://www.youtube.com/@AWSStudyGroup/ https://cloudjourney.awsstudygroup.com/ Fri - Midterm exam - Meeting to adjust some project functions 31/10/2025 31/10/2025 Week 8 Achievements: Completed and integrated AI Chatbot:\nRevised and optimized AI Chatbot Successfully integrated Chatbot into final project Completed architecture and project documentation:\nRevised system architecture diagram Completed Project Proposal Application development:\nCompleted basic Frontend design (user interface) Completed basic Backend design (processing logic) Review and midterm exam:\nReviewed all AWS knowledge learned in 8 weeks Supplemented knowledge from AWS Study Group and Cloud Journey Successfully participated in midterm exam Team collaboration:\nRegular meetings to coordinate project progress Agreed on direction for function adjustment and improvement "},{"uri":"https://thanhtai15.github.io/Inter_report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Learn AWS Bedrock and apply it to the project: Explore Bedrock architecture and Foundation Models Research models: Claude, Llama, Titan, Jurassic Learn effective prompt engineering techniques Integrate Bedrock API into application Compare Bedrock with other AI solutions Research necessary NLP knowledge - Deep Learning for NLP: Detailed Transformer architecture (Attention is All You Need) Compare Transformers vs RNNs/LSTMs Scaled Dot-Product Attention and Multi-Head Attention Masked Self-Attention in Decoder Positional Encoding and its importance Famous models: BERT, GPT, T5, ELMo Practice LeetCode to improve programming skills: Solve problems about String manipulation Dynamic Programming algorithms Data structures: Arrays, Hash Tables, Trees Improve problem-solving and coding speed Tasks to implement this week: Day Task Start Date End Date Resources Mon Study Transformer material Practice LeetCode exercises. 03/11/2025 03/11/2025 https://www.youtube.com/watch?v=qTXwP-5_1Lw https://www.youtube.com/watch?v=zxQyTK8quyY\u0026t=1s Tue Study Bedrock material 04/11/2025 04/11/2025 https://www.youtube.com/watch?v=ab1mbj0acDo https://www.youtube.com/watch?v=7NJDhGO0aWs\u0026t=2s Wed Continue detailed research on Bedrock 05/11/2025 05/11/2025 Thu Practice LeetCode exercises. 06/11/2025 06/11/2025 Fri Review and prepare for next week\u0026rsquo;s research 07/11/2025 07/11/2025 Week 9 Achievements: Learned about Transformer and NLP models:\nUnderstood the architecture and operating mechanism of Transformer Studied Encoder-Decoder structure, Attention Mechanism Learned about prominent models: BERT, GPT Applied Transformer knowledge to natural language processing problems Researched and learned about AWS Bedrock:\nUnderstood key concepts and features of Bedrock Learned how to use AWS Bedrock to integrate Large Language Models (LLMs) Researched popular pre-trained models on Bedrock platform Explored practical use cases in projects Algorithm practice:\nCompleted LeetCode exercises to enhance problem-solving skills Improved code optimization and algorithm thinking Project preparation:\nPlanned to apply Bedrock to final project Reviewed architecture to integrate AI models Prepared knowledge foundation for implementation in coming weeks "},{"uri":"https://thanhtai15.github.io/Inter_report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Master advanced NLP techniques - BERT and Transfer Learning: Bidirectional Encoder Representations from Transformers (BERT) architecture Understand Masked Language Modeling (MLM) mechanism Next Sentence Prediction (NSP) task How to Fine-tune BERT for specific tasks: sentiment analysis, NER, Q\u0026amp;A Multi-Task Training Strategy to improve performance Practice fine-tuning BERT with custom dataset Finalize product interface (UI) and testing: Complete all pages and components Implement responsive design for mobile and tablet Add loading states and error handling Test UX flow and collect feedback Conduct comprehensive system testing: Unit testing for functions and components Integration testing for APIs End-to-end testing for user flows Performance testing and load testing Optimize system architecture - Serverless transformation: Shift from Backend monolithic to Serverless Write AWS Lambda functions to handle business logic Connect Frontend directly to Bedrock via Lambda Configure API Gateway and Lambda triggers Optimize cost and performance with serverless architecture Tasks to implement this week: Day Task Start Date End Date Resources Mon - Learn about Bidirectional Encoder Representations from Transformers (BERT) - Learn how to fine-tune the BERT model - Learn about Multi-Task Training Strategy - Complete code lab on fine-tuning BERT model based on available data 10/11/2025 10/11/2025 https://www.coursera.org/learn/attention-models-in-nlp/ Tue - Complete User Interface (UI) - Finalize basic functions - Conduct product testing 11/11/2025 11/11/2025 Wed - Meeting to discuss adding certain features - Optimize the system 12/11/2025 12/11/2025 Thu - System optimization - Fix integration errors related to Bedrock AI ChatBot 13/11/2025 13/11/2025 https://cloudjourney.awsstudygroup.com/ Fri - Write Lambda Function to integrate Bedrock into Front-end instead of Back-end - Research how to integrate Bedrock into the project\u0026rsquo;s Front-end 14/11/2025 14/11/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Advanced NLP Mastery (BERT):\nUnderstood the Bidirectional Encoder Representations from Transformers (BERT) architecture. Successfully applied Multi-Task Training Strategies. Completed the lab on Fine-tuning BERT with custom datasets to improve model accuracy. Product Finalization:\nCompleted the User Interface (UI) and verified all core functionalities. Executed a full testing phase to identify and resolve bugs. Architectural Refactoring \u0026amp; Optimization:\nTransitioned the AI Chatbot integration from a monolithic backend approach to a Serverless architecture. Developed AWS Lambda functions to allow the Frontend to communicate directly with AWS Bedrock, improving latency and scalability. Optimized overall system performance based on team feedback. "},{"uri":"https://thanhtai15.github.io/Inter_report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Write comprehensive Internship Report (OJT Report): Overview of 11-week internship process Describe work and achievements each week AWS skills and knowledge learned Challenges faced and solutions Final project results and lessons learned Complete and submit team Proposal: Edit and finalize proposal content Add diagrams and actual screenshots Review and proofread all documents Format properly and export to PDF for submission Synchronize architecture according to mentor\u0026rsquo;s feedback: Adjust architecture diagram based on feedback Update AWS services currently in use Ensure architecture reflects 100% actual system Document architecture decisions and trade-offs Deploy complete application on AWS: Configure all necessary AWS services Deploy Frontend to S3 + CloudFront or Amplify Deploy Backend/Lambda functions Configure RDS/DynamoDB for database Setup CI/CD pipeline with CodePipeline or GitHub Actions Test application on production environment Tasks to implement this week: Day Task Start Date End Date Resources Mon - Write OJT report - Finish team proposal 17/11/2025 17/11/2025 Tue - Complete weekly reports - Synchronize architecture according to mentor\u0026rsquo;s feedback 18/11/2025 18/11/2025 Wed - Prepare for deployment on AWS 19/11/2025 19/11/2025 Thu - Deploy complete application on AWS 20/11/2025 20/11/2025 Fri - Test application and fix bugs 21/11/2025 21/11/2025 Week 11 Achievements: Completed required documentation:\nCompleted and submitted OJT Report summarizing internship process Completed all Weekly Reports (Week 1-11) Completed and submitted team Project Proposal Improved project architecture:\nRevised and synchronized architecture diagram according to mentor\u0026rsquo;s feedback Optimized system structure to meet actual requirements Ensured clear and logical architecture Successfully deployed on AWS:\nCompleted setup of all AWS services for project Successfully deployed complete application on AWS cloud environment Configured networking, security, and scalability Application testing and improvement:\nTested all functions on AWS environment Fixed bugs discovered during testing Optimized application performance Prepared for final presentation:\nCompleted all necessary documentation Application running stably on AWS Ready for demo and project presentation "},{"uri":"https://thanhtai15.github.io/Inter_report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Final testing and project presentation: Perform full system testing on AWS production Check all user flows and edge cases Performance testing and security audit Fix all remaining bugs and issues Prepare demo and presentation materials Complete comprehensive project documentation: Technical Documentation: System architecture, API docs User Guide: Instructions for end-users Deployment Guide: Deployment steps and configuration README.md with setup instructions Code comments and inline documentation Practice presentation: Prepare professional slides with PowerPoint/Google Slides Write clear, logical presentation script Practice product demo with team Prepare answers for evaluation board questions Submit final report and complete OJT: Complete Final Report summarizing 12 weeks Submit all required documents Receive feedback and evaluation from mentor Summarize experiences and lessons learned Tasks to implement this week: Day Task Start Date End Date Resources Mon - Final testing - Fix bugs 24/11/2025 24/11/2025 Tue - Complete project documentation - Practice presentation 25/11/2025 25/11/2025 Wed - Prepare materials for presentation 26/11/2025 26/11/2025 Thu - Final project presentation 27/11/2025 27/11/2025 Fri - Submit final report - Complete OJT internship 28/11/2025 28/11/2025 Week 12 Achievements: Completed final testing:\nConducted comprehensive system testing Fixed all bugs discovered Ensured application runs smoothly and stably Finalized project documentation:\nCompleted all technical documentation Updated API documentation Completed deployment and user guides Successful project presentation:\nPrepared professional presentation materials Successfully presented project to mentors and evaluation board Clearly demonstrated system features and operation Completed internship:\nSubmitted complete final report Summarized all learning outcomes during OJT period Successfully completed AWS internship program Key achievements:\nMastered AWS services: EC2, S3, VPC, RDS, Lambda, Bedrock Developed and deployed complete AI application on AWS Learned NLP and machine learning model integration Developed teamwork and project management skills Achieved all OJT program objectives "},{"uri":"https://thanhtai15.github.io/Inter_report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://thanhtai15.github.io/Inter_report/tags/","title":"Tags","tags":[],"description":"","content":""}]